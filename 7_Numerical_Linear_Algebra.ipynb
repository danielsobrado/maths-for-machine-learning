{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bef1d65c",
   "metadata": {},
   "source": [
    "# Numerical Linear Algebra for Machine Learning\n",
    "## Computational Efficiency and Large-Scale Operations\n",
    "\n",
    "Welcome to the **computational heart** of machine learning! While theoretical linear algebra tells us what's possible, numerical linear algebra makes it **fast and practical** at scale.\n",
    "\n",
    "### What You'll Master\n",
    "By the end of this notebook, you'll understand:\n",
    "1. **Numerical stability** - Why some algorithms fail in practice\n",
    "2. **Matrix decompositions** - The building blocks of efficient computation\n",
    "3. **Iterative methods** - Solving huge systems without storing full matrices\n",
    "4. **Computational complexity** - Why some operations scale better than others\n",
    "5. **Memory optimization** - Making algorithms work with limited resources\n",
    "6. **GPU acceleration** - Parallel computing for massive speedups\n",
    "\n",
    "### Why This is Critical\n",
    "- **Modern AI models** have billions of parameters\n",
    "- **Naive algorithms** would take centuries to train large models\n",
    "- **Numerical methods** make the impossible practical\n",
    "- **Understanding efficiency** helps you design scalable systems\n",
    "\n",
    "### Real-World Impact\n",
    "- **GPT models**: Trained using optimized matrix operations\n",
    "- **Recommendation systems**: Handle millions of users efficiently\n",
    "- **Computer vision**: Process high-resolution images in real-time\n",
    "\n",
    "Let's make machine learning lightning fast! ‚ö°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f035f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy.linalg import solve, lstsq, svd, qr, cholesky, lu\n",
    "from scipy.sparse.linalg import spsolve, cg, gmres\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For GPU operations (if available)\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"üöÄ GPU acceleration available with CuPy!\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"üíª Using CPU computation (install CuPy for GPU acceleration)\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚ö° Numerical Linear Algebra toolkit loaded!\")\n",
    "print(\"Ready for high-performance computing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf864e",
   "metadata": {},
   "source": [
    "## 1. Computational Complexity: Understanding the Cost\n",
    "\n",
    "### Why Complexity Matters\n",
    "In machine learning, we often deal with:\n",
    "- **Massive datasets**: Millions of samples\n",
    "- **High dimensions**: Thousands of features\n",
    "- **Deep networks**: Billions of parameters\n",
    "\n",
    "A naive algorithm might work for small problems but **fail completely** at scale!\n",
    "\n",
    "### Big O Notation Refresher\n",
    "- **O(n)**: Linear time - doubles when data doubles\n",
    "- **O(n¬≤)**: Quadratic time - 4x slower when data doubles\n",
    "- **O(n¬≥)**: Cubic time - 8x slower when data doubles\n",
    "- **O(log n)**: Logarithmic time - barely slower as data grows\n",
    "\n",
    "### Common Linear Algebra Operations\n",
    "\n",
    "| Operation | Naive Complexity | Optimized Complexity | Memory |\n",
    "|-----------|------------------|---------------------|--------|\n",
    "| Matrix-Vector | O(n¬≤) | O(n¬≤) | O(n¬≤) |\n",
    "| Matrix-Matrix | O(n¬≥) | O(n^2.376) | O(n¬≤) |\n",
    "| Matrix Inverse | O(n¬≥) | O(n¬≥) | O(n¬≤) |\n",
    "| SVD | O(mn¬≤) | O(mn¬≤) | O(mn) |\n",
    "| Eigenvalues | O(n¬≥) | O(n¬≥) | O(n¬≤) |\n",
    "\n",
    "### The Scalability Crisis\n",
    "For a matrix of size n√ón:\n",
    "- **n = 1,000**: Matrix multiplication takes ~1 second\n",
    "- **n = 10,000**: Matrix multiplication takes ~17 minutes\n",
    "- **n = 100,000**: Matrix multiplication takes ~12 days!\n",
    "\n",
    "**Solution**: Use specialized algorithms and avoid unnecessary operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matrix_operations():\n",
    "    \"\"\"Benchmark common matrix operations at different scales\"\"\"\n",
    "    \n",
    "    sizes = [100, 500, 1000, 2000]\n",
    "    operations = ['Matrix-Vector', 'Matrix-Matrix', 'Matrix Inverse', 'SVD']\n",
    "    \n",
    "    results = {op: [] for op in operations}\n",
    "    \n",
    "    print(\"‚è±Ô∏è  Benchmarking Matrix Operations\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Size':<6} {'Mat-Vec':<10} {'Mat-Mat':<10} {'Inverse':<10} {'SVD':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in sizes:\n",
    "        # Generate random matrices\n",
    "        A = np.random.randn(size, size)\n",
    "        B = np.random.randn(size, size)\n",
    "        x = np.random.randn(size)\n",
    "        \n",
    "        # Matrix-Vector multiplication\n",
    "        start = time.time()\n",
    "        _ = A @ x\n",
    "        mat_vec_time = time.time() - start\n",
    "        results['Matrix-Vector'].append(mat_vec_time)\n",
    "        \n",
    "        # Matrix-Matrix multiplication\n",
    "        start = time.time()\n",
    "        _ = A @ B\n",
    "        mat_mat_time = time.time() - start\n",
    "        results['Matrix-Matrix'].append(mat_mat_time)\n",
    "        \n",
    "        # Matrix inverse (if not too large)\n",
    "        if size <= 1000:\n",
    "            start = time.time()\n",
    "            _ = np.linalg.inv(A)\n",
    "            inv_time = time.time() - start\n",
    "            results['Matrix Inverse'].append(inv_time)\n",
    "        else:\n",
    "            results['Matrix Inverse'].append(np.nan)\n",
    "            inv_time = float('inf')\n",
    "        \n",
    "        # SVD (if not too large)\n",
    "        if size <= 1000:\n",
    "            start = time.time()\n",
    "            _ = np.linalg.svd(A)\n",
    "            svd_time = time.time() - start\n",
    "            results['SVD'].append(svd_time)\n",
    "        else:\n",
    "            results['SVD'].append(np.nan)\n",
    "            svd_time = float('inf')\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"{size:<6} {mat_vec_time:<10.4f} {mat_mat_time:<10.4f} \",\n",
    "              f\"{inv_time if inv_time != float('inf') else 'N/A':<10} \",\n",
    "              f\"{svd_time if svd_time != float('inf') else 'N/A':<10}\")\n",
    "    \n",
    "    # Plot scaling behavior\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (op, times) in enumerate(results.items()):\n",
    "        valid_sizes = []\n",
    "        valid_times = []\n",
    "        \n",
    "        for j, t in enumerate(times):\n",
    "            if not np.isnan(t):\n",
    "                valid_sizes.append(sizes[j])\n",
    "                valid_times.append(t)\n",
    "        \n",
    "        if valid_times:\n",
    "            axes[i].loglog(valid_sizes, valid_times, 'o-', linewidth=2, markersize=8)\n",
    "            axes[i].set_xlabel('Matrix Size (n)')\n",
    "            axes[i].set_ylabel('Time (seconds)')\n",
    "            axes[i].set_title(f'{op} Scaling')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add theoretical scaling lines\n",
    "            if op == 'Matrix-Vector':\n",
    "                theoretical = [(s/valid_sizes[0])**2 * valid_times[0] for s in valid_sizes]\n",
    "                axes[i].loglog(valid_sizes, theoretical, '--', alpha=0.7, label='O(n¬≤) theoretical')\n",
    "            elif op == 'Matrix-Matrix':\n",
    "                theoretical = [(s/valid_sizes[0])**3 * valid_times[0] for s in valid_sizes]\n",
    "                axes[i].loglog(valid_sizes, theoretical, '--', alpha=0.7, label='O(n¬≥) theoretical')\n",
    "            \n",
    "            axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_results = benchmark_matrix_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bd30c5",
   "metadata": {},
   "source": [
    "## 2. Matrix Decompositions: The Workhorses of Numerical Computing\n",
    "\n",
    "### Why Decompositions?\n",
    "Instead of working with the original matrix, we **decompose** it into simpler parts:\n",
    "- **Easier to compute** with\n",
    "- **More numerically stable**\n",
    "- **Reveal mathematical structure**\n",
    "- **Enable efficient algorithms**\n",
    "\n",
    "### Key Decompositions in ML\n",
    "\n",
    "#### 1. LU Decomposition: A = LU\n",
    "- **L**: Lower triangular matrix\n",
    "- **U**: Upper triangular matrix\n",
    "- **Use**: Solving linear systems efficiently\n",
    "\n",
    "#### 2. QR Decomposition: A = QR\n",
    "- **Q**: Orthogonal matrix (Q^T Q = I)\n",
    "- **R**: Upper triangular matrix\n",
    "- **Use**: Linear regression, least squares\n",
    "\n",
    "#### 3. SVD: A = UŒ£V^T\n",
    "- **U, V**: Orthogonal matrices\n",
    "- **Œ£**: Diagonal matrix (singular values)\n",
    "- **Use**: Dimensionality reduction, pseudoinverse\n",
    "\n",
    "#### 4. Eigendecomposition: A = QŒõQ^T\n",
    "- **Q**: Eigenvectors\n",
    "- **Œõ**: Eigenvalues (diagonal)\n",
    "- **Use**: PCA, spectral methods\n",
    "\n",
    "#### 5. Cholesky: A = LL^T\n",
    "- **L**: Lower triangular matrix\n",
    "- **Requirement**: A must be positive definite\n",
    "- **Use**: Gaussian processes, covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d690416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_matrix_decompositions():\n",
    "    \"\"\"Show different matrix decompositions and their properties\"\"\"\n",
    "    \n",
    "    print(\"üîß Matrix Decomposition Showcase\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create a test matrix\n",
    "    np.random.seed(42)\n",
    "    n = 5\n",
    "    A = np.random.randn(n, n)\n",
    "    A = A + A.T  # Make symmetric for some decompositions\n",
    "    A = A + n * np.eye(n)  # Make positive definite\n",
    "    \n",
    "    print(f\"Original matrix A ({n}x{n}):\")\n",
    "    print(A)\n",
    "    print(f\"Condition number: {np.linalg.cond(A):.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # 1. LU Decomposition\n",
    "    print(\"1. LU Decomposition: A = LU\")\n",
    "    P, L, U = lu(A)\n",
    "    print(f\"   Reconstruction error: {np.linalg.norm(P @ L @ U - A):.2e}\")\n",
    "    print(f\"   L is lower triangular: {np.allclose(L, np.tril(L))}\")\n",
    "    print(f\"   U is upper triangular: {np.allclose(U, np.triu(U))}\")\n",
    "    print()\n",
    "    \n",
    "    # 2. QR Decomposition\n",
    "    print(\"2. QR Decomposition: A = QR\")\n",
    "    Q, R = qr(A)\n",
    "    print(f\"   Reconstruction error: {np.linalg.norm(Q @ R - A):.2e}\")\n",
    "    print(f\"   Q is orthogonal: {np.linalg.norm(Q.T @ Q - np.eye(n)):.2e}\")\n",
    "    print(f\"   R is upper triangular: {np.allclose(R, np.triu(R))}\")\n",
    "    print()\n",
    "    \n",
    "    # 3. SVD\n",
    "    print(\"3. Singular Value Decomposition: A = UŒ£V^T\")\n",
    "    U, s, Vt = svd(A)\n",
    "    print(f\"   Reconstruction error: {np.linalg.norm(U @ np.diag(s) @ Vt - A):.2e}\")\n",
    "    print(f\"   Singular values: {s}\")\n",
    "    print(f\"   Rank: {np.sum(s > 1e-10)}\")\n",
    "    print()\n",
    "    \n",
    "    # 4. Eigendecomposition\n",
    "    print(\"4. Eigendecomposition: A = QŒõQ^T\")\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "    A_reconstructed = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n",
    "    print(f\"   Reconstruction error: {np.linalg.norm(A_reconstructed - A):.2e}\")\n",
    "    print(f\"   Eigenvalues: {eigenvalues.real}\")\n",
    "    print(f\"   All eigenvalues positive: {np.all(eigenvalues.real > 0)}\")\n",
    "    print()\n",
    "    \n",
    "    # 5. Cholesky Decomposition\n",
    "    print(\"5. Cholesky Decomposition: A = LL^T\")\n",
    "    try:\n",
    "        L_chol = cholesky(A, lower=True)\n",
    "        print(f\"   Reconstruction error: {np.linalg.norm(L_chol @ L_chol.T - A):.2e}\")\n",
    "        print(f\"   L is lower triangular: {np.allclose(L_chol, np.tril(L_chol))}\")\n",
    "        print(f\"   Computational advantage: Only need to store lower triangle\")\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"   Matrix is not positive definite - Cholesky not applicable\")\n",
    "    print()\n",
    "    \n",
    "    return A, (P, L, U), (Q, R), (U, s, Vt), (eigenvalues, eigenvectors)\n",
    "\n",
    "# Demonstrate decompositions\n",
    "decomp_results = demonstrate_matrix_decompositions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe397fd",
   "metadata": {},
   "source": [
    "## 3. Solving Linear Systems: From Direct to Iterative Methods\n",
    "\n",
    "### The Fundamental Problem\n",
    "In machine learning, we constantly solve systems like:\n",
    "```\n",
    "Ax = b\n",
    "```\n",
    "Where:\n",
    "- **A**: Coefficient matrix (often large and sparse)\n",
    "- **x**: Unknown vector (what we want to find)\n",
    "- **b**: Right-hand side vector (known)\n",
    "\n",
    "### Method Categories\n",
    "\n",
    "#### Direct Methods\n",
    "- **Solve exactly** in finite steps\n",
    "- **Examples**: Gaussian elimination, LU decomposition\n",
    "- **Pros**: Exact solution (up to machine precision)\n",
    "- **Cons**: O(n¬≥) complexity, requires storing full matrix\n",
    "\n",
    "#### Iterative Methods\n",
    "- **Approximate solution** through iteration\n",
    "- **Examples**: Conjugate gradient, GMRES\n",
    "- **Pros**: Memory efficient, good for sparse matrices\n",
    "- **Cons**: May not converge, approximate solution\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "| Matrix Type | Size | Best Method | Why |\n",
    "|-------------|------|-------------|-----|\n",
    "| Dense, small | < 1000 | Direct (LU) | Fast and exact |\n",
    "| Dense, large | > 10000 | Iterative | Memory constraints |\n",
    "| Sparse, any | Any | Iterative | Exploits sparsity |\n",
    "| Positive definite | Any | Cholesky/CG | Numerical stability |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
