{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079a52ce",
   "metadata": {},
   "source": [
    "# Optimization Theory for Machine Learning\n",
    "## From Gradient Descent to Advanced Convex Optimization\n",
    "\n",
    "Welcome to the mathematical foundation of **how machines learn**! Every time an AI system improves its performance, optimization algorithms are working behind the scenes.\n",
    "\n",
    "### What You'll Master\n",
    "By the end of this notebook, you'll understand:\n",
    "1. **Optimization fundamentals** - What it means to find the \"best\" solution\n",
    "2. **Gradient descent variants** - The workhorses of machine learning\n",
    "3. **Convex optimization** - When we can guarantee finding global optima\n",
    "4. **Constrained optimization** - Real-world problems with restrictions\n",
    "5. **Advanced algorithms** - Modern techniques for deep learning\n",
    "\n",
    "### Why This Matters\n",
    "- **Every ML algorithm** uses optimization to learn from data\n",
    "- **Neural networks** train using sophisticated optimization methods\n",
    "- **Understanding optimization** helps you debug and improve models\n",
    "- **Convexity** tells you when you can trust your solution\n",
    "\n",
    "Let's optimize our understanding! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033eb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize, minimize_scalar\n",
    "from scipy.optimize import differential_evolution\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for beautiful plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üéØ Optimization toolkit loaded successfully!\")\n",
    "print(\"Ready to find the best solutions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891eeaf4",
   "metadata": {},
   "source": [
    "## 1. Optimization Fundamentals: The Quest for the Best\n",
    "\n",
    "### What is Optimization?\n",
    "**Optimization** is the mathematical process of finding the **best solution** from a set of possible solutions.\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "```\n",
    "minimize   f(x)\n",
    "subject to x ‚àà D\n",
    "```\n",
    "Where:\n",
    "- `f(x)` is the **objective function** (what we want to minimize)\n",
    "- `x` is the **decision variable** (what we can control)\n",
    "- `D` is the **feasible region** (allowed values of x)\n",
    "\n",
    "### Types of Optimization Problems\n",
    "\n",
    "1. **Unconstrained**: No restrictions on x\n",
    "2. **Constrained**: x must satisfy certain conditions\n",
    "3. **Convex**: Guarantee global optimum (the holy grail!)\n",
    "4. **Non-convex**: May have multiple local optima\n",
    "\n",
    "### Real-World Analogy: Finding the Lowest Point\n",
    "Imagine you're blindfolded on a hilly landscape and need to find the lowest valley:\n",
    "- **Local minimum**: Lowest point in your immediate area\n",
    "- **Global minimum**: The absolute lowest point in the entire landscape\n",
    "- **Gradient**: The slope that tells you which direction is downhill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize different types of optimization landscapes\n",
    "\n",
    "def create_optimization_landscapes():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = np.linspace(-3, 3, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # 1. Convex function (bowl shape) - EASY to optimize\n",
    "    Z1 = X**2 + Y**2\n",
    "    ax1 = axes[0, 0]\n",
    "    contour1 = ax1.contour(X, Y, Z1, levels=15, alpha=0.6)\n",
    "    ax1.clabel(contour1, inline=True, fontsize=8)\n",
    "    ax1.scatter([0], [0], color='red', s=100, zorder=5, marker='*')\n",
    "    ax1.set_title('Convex Function: f(x,y) = x¬≤ + y¬≤\\n‚úÖ One Global Minimum')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Non-convex with multiple local minima - HARD to optimize\n",
    "    Z2 = (X**2 + Y**2) * np.exp(-(X**2 + Y**2)) + 0.1 * np.sin(5*X) * np.sin(5*Y)\n",
    "    ax2 = axes[0, 1]\n",
    "    contour2 = ax2.contour(X, Y, Z2, levels=20, alpha=0.6)\n",
    "    ax2.clabel(contour2, inline=True, fontsize=8)\n",
    "    ax2.set_title('Non-Convex Function\\n‚ö†Ô∏è Multiple Local Minima')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('y')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Saddle point - TRICKY to optimize\n",
    "    Z3 = X**2 - Y**2\n",
    "    ax3 = axes[1, 0]\n",
    "    contour3 = ax3.contour(X, Y, Z3, levels=15, alpha=0.6)\n",
    "    ax3.clabel(contour3, inline=True, fontsize=8)\n",
    "    ax3.scatter([0], [0], color='orange', s=100, zorder=5, marker='s')\n",
    "    ax3.set_title('Saddle Point: f(x,y) = x¬≤ - y¬≤\\nüèîÔ∏è Not a Minimum or Maximum')\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Rosenbrock function - CLASSIC optimization benchmark\n",
    "    a, b = 1, 100\n",
    "    Z4 = (a - X)**2 + b * (Y - X**2)**2\n",
    "    Z4_log = np.log(Z4 + 1)  # Log scale for better visualization\n",
    "    ax4 = axes[1, 1]\n",
    "    contour4 = ax4.contour(X, Y, Z4_log, levels=20, alpha=0.6)\n",
    "    ax4.clabel(contour4, inline=True, fontsize=8)\n",
    "    ax4.scatter([1], [1], color='red', s=100, zorder=5, marker='*')\n",
    "    ax4.set_title('Rosenbrock Function (log scale)\\nüåπ Classic Optimization Challenge')\n",
    "    ax4.set_xlabel('x')\n",
    "    ax4.set_ylabel('y')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"üéØ Optimization Landscape Types:\")\n",
    "    print(\"1. Convex: Gradient descent ALWAYS finds global minimum\")\n",
    "    print(\"2. Non-convex: May get stuck in local minima\")\n",
    "    print(\"3. Saddle points: Gradients can get confused\")\n",
    "    print(\"4. Rosenbrock: Tests algorithm robustness\")\n",
    "\n",
    "create_optimization_landscapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44bb1e",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent: The Workhorse of Machine Learning\n",
    "\n",
    "### The Big Idea\n",
    "**Gradient descent** is like rolling a ball down a hill - it naturally finds the bottom by following the steepest path downward.\n",
    "\n",
    "### The Algorithm\n",
    "```\n",
    "1. Start at random point x‚ÇÄ\n",
    "2. Compute gradient ‚àáf(x)\n",
    "3. Take step: x_{new} = x_{old} - Œ±‚àáf(x)\n",
    "4. Repeat until convergence\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "- **Learning Rate (Œ±)**: How big steps to take\n",
    "  - Too small ‚Üí slow convergence\n",
    "  - Too large ‚Üí overshoot minimum\n",
    "  - Just right ‚Üí efficient convergence\n",
    "\n",
    "### Variants of Gradient Descent\n",
    "\n",
    "1. **Batch Gradient Descent**: Uses entire dataset\n",
    "2. **Stochastic Gradient Descent (SGD)**: Uses one sample at a time\n",
    "3. **Mini-batch Gradient Descent**: Uses small batches\n",
    "4. **Momentum**: Adds inertia to overcome local minima\n",
    "5. **Adam**: Adaptive learning rates (very popular!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of various gradient descent algorithms\n",
    "\n",
    "class OptimizationAlgorithms:\n",
    "    \"\"\"Collection of optimization algorithms for comparison\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def function_2d(x, y):\n",
    "        \"\"\"Test function: f(x,y) = x¬≤ + y¬≤ (simple convex)\"\"\"\n",
    "        return x**2 + y**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_2d(x, y):\n",
    "        \"\"\"Gradient of test function\"\"\"\n",
    "        return np.array([2*x, 2*y])\n",
    "    \n",
    "    @staticmethod\n",
    "    def rosenbrock(x, y, a=1, b=100):\n",
    "        \"\"\"Rosenbrock function - classic optimization benchmark\"\"\"\n",
    "        return (a - x)**2 + b * (y - x**2)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def rosenbrock_gradient(x, y, a=1, b=100):\n",
    "        \"\"\"Gradient of Rosenbrock function\"\"\"\n",
    "        dx = -2*(a - x) - 4*b*x*(y - x**2)\n",
    "        dy = 2*b*(y - x**2)\n",
    "        return np.array([dx, dy])\n",
    "    \n",
    "    def gradient_descent(self, start_x, start_y, learning_rate=0.01, max_iter=1000, func_type='simple'):\n",
    "        \"\"\"Basic gradient descent implementation\"\"\"\n",
    "        path = [(start_x, start_y)]\n",
    "        x, y = start_x, start_y\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            if func_type == 'simple':\n",
    "                grad = self.gradient_2d(x, y)\n",
    "            else:  # rosenbrock\n",
    "                grad = self.rosenbrock_gradient(x, y)\n",
    "            \n",
    "            # Update parameters\n",
    "            x -= learning_rate * grad[0]\n",
    "            y -= learning_rate * grad[1]\n",
    "            path.append((x, y))\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(grad) < 1e-6:\n",
    "                break\n",
    "                \n",
    "        return np.array(path)\n",
    "    \n",
    "    def momentum_gradient_descent(self, start_x, start_y, learning_rate=0.01, momentum=0.9, max_iter=1000, func_type='simple'):\n",
    "        \"\"\"Gradient descent with momentum\"\"\"\n",
    "        path = [(start_x, start_y)]\n",
    "        x, y = start_x, start_y\n",
    "        vx, vy = 0, 0  # Velocity terms\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            if func_type == 'simple':\n",
    "                grad = self.gradient_2d(x, y)\n",
    "            else:  # rosenbrock\n",
    "                grad = self.rosenbrock_gradient(x, y)\n",
    "            \n",
    "            # Update velocity (momentum)\n",
    "            vx = momentum * vx - learning_rate * grad[0]\n",
    "            vy = momentum * vy - learning_rate * grad[1]\n",
    "            \n",
    "            # Update parameters\n",
    "            x += vx\n",
    "            y += vy\n",
    "            path.append((x, y))\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(grad) < 1e-6:\n",
    "                break\n",
    "                \n",
    "        return np.array(path)\n",
    "    \n",
    "    def adam_optimizer(self, start_x, start_y, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, max_iter=1000, func_type='simple'):\n",
    "        \"\"\"Adam optimizer implementation\"\"\"\n",
    "        path = [(start_x, start_y)]\n",
    "        x, y = start_x, start_y\n",
    "        m1, m2 = 0, 0  # First moment\n",
    "        v1, v2 = 0, 0  # Second moment\n",
    "        \n",
    "        for t in range(1, max_iter + 1):\n",
    "            if func_type == 'simple':\n",
    "                grad = self.gradient_2d(x, y)\n",
    "            else:  # rosenbrock\n",
    "                grad = self.rosenbrock_gradient(x, y)\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            m1 = beta1 * m1 + (1 - beta1) * grad[0]\n",
    "            m2 = beta1 * m2 + (1 - beta1) * grad[1]\n",
    "            \n",
    "            # Update biased second raw moment estimate\n",
    "            v1 = beta2 * v1 + (1 - beta2) * grad[0]**2\n",
    "            v2 = beta2 * v2 + (1 - beta2) * grad[1]**2\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m1_hat = m1 / (1 - beta1**t)\n",
    "            m2_hat = m2 / (1 - beta1**t)\n",
    "            \n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            v1_hat = v1 / (1 - beta2**t)\n",
    "            v2_hat = v2 / (1 - beta2**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            x -= learning_rate * m1_hat / (np.sqrt(v1_hat) + epsilon)\n",
    "            y -= learning_rate * m2_hat / (np.sqrt(v2_hat) + epsilon)\n",
    "            path.append((x, y))\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.linalg.norm(grad) < 1e-6:\n",
    "                break\n",
    "                \n",
    "        return np.array(path)\n",
    "\n",
    "# Test the algorithms\n",
    "optimizer = OptimizationAlgorithms()\n",
    "print(\"üöÄ Optimization algorithms ready for testing!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
