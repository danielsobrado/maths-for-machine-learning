{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d918ad9",
   "metadata": {},
   "source": [
    "# Matrix Analysis and Matrix Calculus\n",
    "## The Mathematical Foundation of Neural Network Backpropagation\n",
    "\n",
    "Welcome to the **mathematical engine** that powers deep learning! Matrix calculus is what makes it possible to train neural networks with millions of parameters efficiently.\n",
    "\n",
    "### What You'll Master\n",
    "By the end of this notebook, you'll understand:\n",
    "1. **Matrix derivatives** - How to take derivatives of matrix expressions\n",
    "2. **The chain rule** for matrices - The foundation of backpropagation\n",
    "3. **Jacobian and Hessian matrices** - Higher-order derivatives\n",
    "4. **Automatic differentiation** - How modern frameworks compute gradients\n",
    "5. **Backpropagation algorithm** - Step-by-step neural network training\n",
    "\n",
    "### Why This is Revolutionary\n",
    "- **Neural networks** have millions of parameters organized in matrices\n",
    "- **Matrix calculus** lets us compute all gradients simultaneously\n",
    "- **Backpropagation** trains deep networks efficiently using matrix chain rule\n",
    "- **Understanding this** helps you debug and design better architectures\n",
    "\n",
    "### Prerequisites\n",
    "- Basic linear algebra (vectors, matrices, multiplication)\n",
    "- Multivariate calculus (partial derivatives, chain rule)\n",
    "- Understanding of neural network basics\n",
    "\n",
    "Let's unlock the mathematics of deep learning! üß†üî¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafc469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.linalg import norm, svd\n",
    "import sympy as sp\n",
    "from sympy import symbols, Matrix, diff, simplify\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set style for beautiful plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable symbolic computation for matrix calculus\n",
    "sp.init_printing(use_latex=True)\n",
    "\n",
    "print(\"üß† Matrix Calculus toolkit loaded!\")\n",
    "print(\"Ready to compute gradients like a neural network!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec92808",
   "metadata": {},
   "source": [
    "## 1. Matrix Derivatives: Beyond Scalar Calculus\n",
    "\n",
    "### The Challenge\n",
    "In neural networks, we don't have simple functions like `f(x) = x¬≤`. Instead, we have **matrix functions** like:\n",
    "- Weight matrices: `W ‚àà ‚Ñù·µêÀ£‚Åø`\n",
    "- Activation functions: `œÉ(Wx + b)`\n",
    "- Loss functions: `L(Œ∏) = ||y - ≈∑||¬≤`\n",
    "\n",
    "### Matrix Derivative Notation\n",
    "\n",
    "**Scalar by Vector**: `‚àÇf/‚àÇx` where f is scalar, x is vector\n",
    "```\n",
    "‚àÇf/‚àÇx = [‚àÇf/‚àÇx‚ÇÅ, ‚àÇf/‚àÇx‚ÇÇ, ..., ‚àÇf/‚àÇx‚Çô]·µÄ\n",
    "```\n",
    "\n",
    "**Scalar by Matrix**: `‚àÇf/‚àÇW` where f is scalar, W is matrix\n",
    "```\n",
    "‚àÇf/‚àÇW = [‚àÇf/‚àÇw‚ÇÅ‚ÇÅ  ‚àÇf/‚àÇw‚ÇÅ‚ÇÇ  ...]\n",
    "         [‚àÇf/‚àÇw‚ÇÇ‚ÇÅ  ‚àÇf/‚àÇw‚ÇÇ‚ÇÇ  ...]\n",
    "         [   ...      ...   ...]\n",
    "```\n",
    "\n",
    "**Vector by Vector**: `‚àÇy/‚àÇx` where both are vectors ‚Üí **Jacobian Matrix**\n",
    "\n",
    "### Why Matrix Derivatives Matter\n",
    "1. **Efficiency**: Compute all parameter gradients simultaneously\n",
    "2. **Neural Networks**: Each layer is a matrix transformation\n",
    "3. **Optimization**: Update millions of parameters at once\n",
    "4. **Automatic Differentiation**: Modern frameworks use these rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with simple matrix derivative examples\n",
    "\n",
    "def demonstrate_matrix_derivatives():\n",
    "    \"\"\"Show basic matrix derivative computations\"\"\"\n",
    "    \n",
    "    print(\"üî¢ Matrix Derivative Examples\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Example 1: Quadratic form\n",
    "    print(\"\\n1. Quadratic Form: f(x) = x^T A x\")\n",
    "    print(\"   Derivative: ‚àÇf/‚àÇx = (A + A^T)x\")\n",
    "    print(\"   If A is symmetric: ‚àÇf/‚àÇx = 2Ax\")\n",
    "    \n",
    "    # Numerical example\n",
    "    A = np.array([[2, 1], [1, 3]])\n",
    "    x = np.array([1, 2])\n",
    "    \n",
    "    f_val = x.T @ A @ x\n",
    "    gradient = (A + A.T) @ x\n",
    "    \n",
    "    print(f\"   A = {A}\")\n",
    "    print(f\"   x = {x}\")\n",
    "    print(f\"   f(x) = {f_val}\")\n",
    "    print(f\"   ‚àáf = {gradient}\")\n",
    "    \n",
    "    # Example 2: Linear transformation\n",
    "    print(\"\\n2. Linear Form: f(x) = a^T x\")\n",
    "    print(\"   Derivative: ‚àÇf/‚àÇx = a\")\n",
    "    \n",
    "    a = np.array([2, -1])\n",
    "    f_linear = a.T @ x\n",
    "    grad_linear = a\n",
    "    \n",
    "    print(f\"   a = {a}\")\n",
    "    print(f\"   f(x) = a^T x = {f_linear}\")\n",
    "    print(f\"   ‚àáf = {grad_linear}\")\n",
    "    \n",
    "    # Example 3: Matrix trace\n",
    "    print(\"\\n3. Matrix Trace: f(W) = tr(W^T A W)\")\n",
    "    print(\"   Derivative: ‚àÇf/‚àÇW = A W + A^T W\")\n",
    "    \n",
    "    W = np.array([[1, 2], [3, 1]])\n",
    "    A_mat = np.array([[1, 0], [0, 2]])\n",
    "    \n",
    "    f_trace = np.trace(W.T @ A_mat @ W)\n",
    "    grad_trace = A_mat @ W + A_mat.T @ W\n",
    "    \n",
    "    print(f\"   W = {W}\")\n",
    "    print(f\"   A = {A_mat}\")\n",
    "    print(f\"   f(W) = {f_trace}\")\n",
    "    print(f\"   ‚àÇf/‚àÇW = \\n{grad_trace}\")\n",
    "\n",
    "demonstrate_matrix_derivatives()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f61f6",
   "metadata": {},
   "source": [
    "## 2. The Chain Rule for Matrices: Heart of Backpropagation\n",
    "\n",
    "### The Power of Matrix Chain Rule\n",
    "In neural networks, we have **composite matrix functions**:\n",
    "```\n",
    "Input ‚Üí Layer 1 ‚Üí Layer 2 ‚Üí ... ‚Üí Output ‚Üí Loss\n",
    "  x   ‚Üí   z‚ÇÅ   ‚Üí   z‚ÇÇ   ‚Üí ... ‚Üí   ≈∑    ‚Üí  L\n",
    "```\n",
    "\n",
    "Each arrow represents a matrix transformation!\n",
    "\n",
    "### Matrix Chain Rule Formula\n",
    "For composite function `L(z‚ÇÇ(z‚ÇÅ(x)))`, the gradient is:\n",
    "```\n",
    "‚àÇL/‚àÇx = (‚àÇL/‚àÇz‚ÇÇ) √ó (‚àÇz‚ÇÇ/‚àÇz‚ÇÅ) √ó (‚àÇz‚ÇÅ/‚àÇx)\n",
    "```\n",
    "\n",
    "Where each `‚àÇz_i/‚àÇz_{i-1}` is a **Jacobian matrix**.\n",
    "\n",
    "### The Jacobian Matrix\n",
    "For vector function `f: ‚Ñù‚Åø ‚Üí ‚Ñù·µê`, the Jacobian is:\n",
    "```\n",
    "J = ‚àÇf/‚àÇx = [‚àÇf‚ÇÅ/‚àÇx‚ÇÅ  ‚àÇf‚ÇÅ/‚àÇx‚ÇÇ  ...  ‚àÇf‚ÇÅ/‚àÇx‚Çô]\n",
    "             [‚àÇf‚ÇÇ/‚àÇx‚ÇÅ  ‚àÇf‚ÇÇ/‚àÇx‚ÇÇ  ...  ‚àÇf‚ÇÇ/‚àÇx‚Çô]\n",
    "             [   ...      ...    ...    ... ]\n",
    "             [‚àÇf‚Çò/‚àÇx‚ÇÅ  ‚àÇf‚Çò/‚àÇx‚ÇÇ  ...  ‚àÇf‚Çò/‚àÇx‚Çô]\n",
    "```\n",
    "\n",
    "### Real-World Analogy: Assembly Line\n",
    "Think of a manufacturing assembly line:\n",
    "1. **Raw materials** (input x)\n",
    "2. **Station 1** transforms materials (z‚ÇÅ = f‚ÇÅ(x))\n",
    "3. **Station 2** processes further (z‚ÇÇ = f‚ÇÇ(z‚ÇÅ))\n",
    "4. **Quality check** measures final product (L = loss function)\n",
    "\n",
    "To improve quality, we need to know how each station affects the final product!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a834bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate matrix chain rule with a simple neural network\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"A minimal neural network to demonstrate matrix calculus\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=2, hidden_size=3, output_size=1):\n",
    "        # Initialize weights randomly\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.5\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.5\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "        # Store intermediate values for backprop\n",
    "        self.z1 = None\n",
    "        self.a1 = None\n",
    "        self.z2 = None\n",
    "        self.a2 = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"Derivative of sigmoid\"\"\"\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Layer 1: z1 = W1 * x + b1\n",
    "        self.z1 = self.W1 @ X + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        \n",
    "        # Layer 2: z2 = W2 * a1 + b2\n",
    "        self.z2 = self.W2 @ self.a1 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Mean squared error loss\"\"\"\n",
    "        return 0.5 * np.mean((y_true - y_pred)**2)\n",
    "    \n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"Backpropagation using matrix chain rule\"\"\"\n",
    "        m = X.shape[1]  # Number of samples\n",
    "        \n",
    "        # Step 1: Gradient of loss w.r.t. output\n",
    "        dL_da2 = -(y_true - y_pred) / m\n",
    "        \n",
    "        # Step 2: Gradient w.r.t. z2 (before activation)\n",
    "        dL_dz2 = dL_da2 * self.sigmoid_derivative(self.z2)\n",
    "        \n",
    "        # Step 3: Gradients for W2 and b2\n",
    "        dL_dW2 = dL_dz2 @ self.a1.T\n",
    "        dL_db2 = np.sum(dL_dz2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Step 4: Gradient w.r.t. a1 (chain rule!)\n",
    "        dL_da1 = self.W2.T @ dL_dz2\n",
    "        \n",
    "        # Step 5: Gradient w.r.t. z1\n",
    "        dL_dz1 = dL_da1 * self.sigmoid_derivative(self.z1)\n",
    "        \n",
    "        # Step 6: Gradients for W1 and b1\n",
    "        dL_dW1 = dL_dz1 @ X.T\n",
    "        dL_db1 = np.sum(dL_dz1, axis=1, keepdims=True)\n",
    "        \n",
    "        return {\n",
    "            'dL_dW2': dL_dW2, 'dL_db2': dL_db2,\n",
    "            'dL_dW1': dL_dW1, 'dL_db1': dL_db1\n",
    "        }\n",
    "    \n",
    "    def compute_jacobians(self, X):\n",
    "        \"\"\"Compute Jacobian matrices for each layer\"\"\"\n",
    "        # Jacobian of layer 1 output w.r.t. input\n",
    "        J1 = np.zeros((self.a1.shape[0], X.shape[0]))\n",
    "        for i in range(self.a1.shape[0]):\n",
    "            for j in range(X.shape[0]):\n",
    "                # ‚àÇa1_i/‚àÇx_j = sigmoid'(z1_i) * W1_ij\n",
    "                J1[i, j] = self.sigmoid_derivative(self.z1[i, 0]) * self.W1[i, j]\n",
    "        \n",
    "        # Jacobian of layer 2 output w.r.t. layer 1 output\n",
    "        J2 = np.zeros((self.a2.shape[0], self.a1.shape[0]))\n",
    "        for i in range(self.a2.shape[0]):\n",
    "            for j in range(self.a1.shape[0]):\n",
    "                # ‚àÇa2_i/‚àÇa1_j = sigmoid'(z2_i) * W2_ij\n",
    "                J2[i, j] = self.sigmoid_derivative(self.z2[i, 0]) * self.W2[i, j]\n",
    "        \n",
    "        return J1, J2\n",
    "\n",
    "# Demonstrate the network\n",
    "print(\"üß† Building a Simple Neural Network\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create network\n",
    "net = SimpleNeuralNetwork(input_size=2, hidden_size=3, output_size=1)\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1.0], [2.0]])  # 2D input\n",
    "y_true = np.array([[0.8]])    # Target output\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Target shape: {y_true.shape}\")\n",
    "print(f\"\\nNetwork architecture:\")\n",
    "print(f\"Input layer: {net.W1.shape[1]} neurons\")\n",
    "print(f\"Hidden layer: {net.W1.shape[0]} neurons\")\n",
    "print(f\"Output layer: {net.W2.shape[0]} neuron(s)\")\n",
    "\n",
    "# Forward pass\n",
    "y_pred = net.forward(X)\n",
    "loss = net.compute_loss(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nForward Pass:\")\n",
    "print(f\"Predicted output: {y_pred.flatten()}\")\n",
    "print(f\"True output: {y_true.flatten()}\")\n",
    "print(f\"Loss: {loss:.6f}\")\n",
    "\n",
    "# Backward pass\n",
    "gradients = net.backward(X, y_true, y_pred)\n",
    "\n",
    "print(f\"\\nBackward Pass - Gradients:\")\n",
    "print(f\"‚àÇL/‚àÇW2 shape: {gradients['dL_dW2'].shape}\")\n",
    "print(f\"‚àÇL/‚àÇW1 shape: {gradients['dL_dW1'].shape}\")\n",
    "print(f\"‚àÇL/‚àÇb2 shape: {gradients['dL_db2'].shape}\")\n",
    "print(f\"‚àÇL/‚àÇb1 shape: {gradients['dL_db1'].shape}\")\n",
    "\n",
    "# Compute Jacobians\n",
    "J1, J2 = net.compute_jacobians(X)\n",
    "print(f\"\\nJacobian Matrices:\")\n",
    "print(f\"J1 (‚àÇa1/‚àÇx) shape: {J1.shape}\")\n",
    "print(f\"J2 (‚àÇa2/‚àÇa1) shape: {J2.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
