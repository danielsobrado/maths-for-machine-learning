{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa191e33",
   "metadata": {},
   "source": [
    "# Functional Analysis for Machine Learning\n",
    "## Kernel Methods and Gaussian Processes\n",
    "\n",
    "Welcome to the **infinite-dimensional world** of functional analysis! This branch of mathematics provides the theoretical foundation for kernel methods, Gaussian processes, and modern deep learning theory.\n",
    "\n",
    "### What You'll Master\n",
    "By the end of this notebook, you'll understand:\n",
    "1. **Normed and metric spaces** - The foundation of distance and convergence\n",
    "2. **Banach and Hilbert spaces** - Complete infinite-dimensional spaces\n",
    "3. **Reproducing Kernel Hilbert Spaces (RKHS)** - The theory behind kernel methods\n",
    "4. **Gaussian processes** - Infinite-dimensional Bayesian models\n",
    "5. **Functional derivatives** - Optimization in function spaces\n",
    "6. **Operators and functionals** - Linear maps between infinite-dimensional spaces\n",
    "\n",
    "### Why This is Transformative\n",
    "- **Kernel methods** - SVMs, Gaussian processes, kernel PCA\n",
    "- **Neural network theory** - Universal approximation theorems\n",
    "- **Optimal transport** - Wasserstein GANs and distributional learning\n",
    "- **Variational inference** - Bayesian deep learning\n",
    "\n",
    "### Real-World Applications\n",
    "- **Computer vision**: Kernel-based image classification\n",
    "- **Gaussian processes**: Uncertainty quantification in ML\n",
    "- **Natural language**: Kernel methods for text classification\n",
    "- **Reinforcement learning**: Function approximation theory\n",
    "\n",
    "Let's explore the beautiful world of infinite dimensions! ‚àû"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import linalg, optimize\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor, GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, WhiteKernel, ConstantKernel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"viridis\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚àû Functional Analysis toolkit loaded!\")\n",
    "print(\"Ready to explore infinite-dimensional spaces!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea069a4",
   "metadata": {},
   "source": [
    "## 1. Normed and Metric Spaces\n",
    "\n",
    "### From Finite to Infinite Dimensions\n",
    "Functional analysis extends linear algebra to **infinite-dimensional spaces** where \"vectors\" are functions.\n",
    "\n",
    "### Metric Spaces\n",
    "A **metric space** (X, d) has a distance function d: X √ó X ‚Üí ‚Ñù satisfying:\n",
    "1. **Positive definite**: d(x, y) ‚â• 0, d(x, y) = 0 ‚ü∫ x = y\n",
    "2. **Symmetric**: d(x, y) = d(y, x)\n",
    "3. **Triangle inequality**: d(x, z) ‚â§ d(x, y) + d(y, z)\n",
    "\n",
    "### Normed Spaces\n",
    "A **normed space** (X, ‚Äñ¬∑‚Äñ) has a norm function ‚Äñ¬∑‚Äñ: X ‚Üí ‚Ñù satisfying:\n",
    "1. **Positive definite**: ‚Äñx‚Äñ ‚â• 0, ‚Äñx‚Äñ = 0 ‚ü∫ x = 0\n",
    "2. **Homogeneity**: ‚ÄñŒ±x‚Äñ = |Œ±|‚Äñx‚Äñ\n",
    "3. **Triangle inequality**: ‚Äñx + y‚Äñ ‚â§ ‚Äñx‚Äñ + ‚Äñy‚Äñ\n",
    "\n",
    "### Important Function Spaces\n",
    "**L·µñ spaces**: ‚Äñf‚Äñ‚Çö = (‚à´|f(x)|·µñ dx)^(1/p)\n",
    "- **L¬π**: Integrable functions\n",
    "- **L¬≤**: Square-integrable functions (energy finite)\n",
    "- **L‚àû**: Essentially bounded functions\n",
    "\n",
    "**C‚Å∞ space**: Continuous functions with sup norm ‚Äñf‚Äñ‚àû = sup|f(x)|\n",
    "\n",
    "### Completeness\n",
    "A space is **complete** if every Cauchy sequence converges:\n",
    "- **Banach space**: Complete normed space\n",
    "- **Hilbert space**: Complete inner product space\n",
    "\n",
    "### Why This Matters for ML\n",
    "- **Function approximation**: Neural networks approximate functions in these spaces\n",
    "- **Regularization**: Norms control function complexity\n",
    "- **Kernel methods**: Work in reproducing kernel Hilbert spaces\n",
    "- **Optimization**: Gradient descent in function spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ef17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_function_spaces():\n",
    "    \"\"\"Explore normed spaces and function approximation\"\"\"\n",
    "    \n",
    "    print(\"üìê Function Spaces: From Finite to Infinite Dimensions\")\n",
    "    print(\"=\" * 54)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Different norms on function spaces\n",
    "    print(\"\\n1. Function Norms: Different Ways to Measure Function Size\")\n",
    "    \n",
    "    # Create test functions\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    dx = x[1] - x[0]\n",
    "    \n",
    "    functions = {\n",
    "        'Smooth': np.sin(2 * np.pi * x),\n",
    "        'Sharp peak': np.exp(-50 * (x - 0.5)**2),\n",
    "        'Discontinuous': np.where(x < 0.5, 1, -1),\n",
    "        'Oscillatory': np.sin(20 * np.pi * x) * np.exp(-x)\n",
    "    }\n",
    "    \n",
    "    # Compute different norms\n",
    "    norms_data = []\n",
    "    \n",
    "    for name, f in functions.items():\n",
    "        # L1 norm (integral of absolute value)\n",
    "        l1_norm = np.sum(np.abs(f)) * dx\n",
    "        \n",
    "        # L2 norm (energy norm)\n",
    "        l2_norm = np.sqrt(np.sum(f**2) * dx)\n",
    "        \n",
    "        # L‚àû norm (supremum norm)\n",
    "        linf_norm = np.max(np.abs(f))\n",
    "        \n",
    "        norms_data.append({\n",
    "            'Function': name,\n",
    "            'L¬π norm': l1_norm,\n",
    "            'L¬≤ norm': l2_norm,\n",
    "            'L‚àû norm': linf_norm\n",
    "        })\n",
    "    \n",
    "    # Plot functions\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(functions)))\n",
    "    \n",
    "    for i, (name, f) in enumerate(functions.items()):\n",
    "        if i < 3:\n",
    "            ax = axes[0, i]\n",
    "            ax.plot(x, f, color=colors[i], linewidth=2)\n",
    "            ax.set_title(f'{name} Function')\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('f(x)')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add norm information\n",
    "            norm_info = norms_data[i]\n",
    "            ax.text(0.05, 0.95, f\"L¬π: {norm_info['L¬π norm']:.3f}\\n\" +\n",
    "                              f\"L¬≤: {norm_info['L¬≤ norm']:.3f}\\n\" +\n",
    "                              f\"L‚àû: {norm_info['L‚àû norm']:.3f}\",\n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Display norms table\n",
    "    norms_df = pd.DataFrame(norms_data)\n",
    "    print(\"   Function norms comparison:\")\n",
    "    print(norms_df.to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    # 2. Function approximation in different spaces\n",
    "    print(\"\\n2. Function Approximation: Polynomial vs Fourier\")\n",
    "    \n",
    "    # Target function\n",
    "    def target_function(x):\n",
    "        return np.sin(3*x) + 0.5*np.cos(7*x) + 0.3*np.sin(15*x)\n",
    "    \n",
    "    x_fine = np.linspace(0, 2*np.pi, 1000)\n",
    "    y_target = target_function(x_fine)\n",
    "    \n",
    "    # Polynomial approximation\n",
    "    n_poly = 10\n",
    "    x_sample = np.linspace(0, 2*np.pi, 20)\n",
    "    y_sample = target_function(x_sample)\n",
    "    \n",
    "    # Fit polynomial\n",
    "    poly_coeffs = np.polyfit(x_sample, y_sample, n_poly)\n",
    "    y_poly = np.polyval(poly_coeffs, x_fine)\n",
    "    \n",
    "    # Fourier approximation\n",
    "    n_fourier = 10\n",
    "    fourier_approx = np.zeros_like(x_fine)\n",
    "    \n",
    "    # Compute Fourier coefficients\n",
    "    for k in range(1, n_fourier + 1):\n",
    "        # Approximate coefficients using samples\n",
    "        a_k = np.mean(y_sample * np.cos(k * x_sample)) * 2\n",
    "        b_k = np.mean(y_sample * np.sin(k * x_sample)) * 2\n",
    "        fourier_approx += a_k * np.cos(k * x_fine) + b_k * np.sin(k * x_fine)\n",
    "    \n",
    "    # Add constant term\n",
    "    fourier_approx += np.mean(y_sample)\n",
    "    \n",
    "    # Plot approximations\n",
    "    axes[1, 0].plot(x_fine, y_target, 'k-', linewidth=2, label='Target function')\n",
    "    axes[1, 0].plot(x_fine, y_poly, 'r--', linewidth=2, label=f'Polynomial (degree {n_poly})')\n",
    "    axes[1, 0].plot(x_sample, y_sample, 'bo', markersize=4, label='Sample points')\n",
    "    axes[1, 0].set_xlabel('x')\n",
    "    axes[1, 0].set_ylabel('f(x)')\n",
    "    axes[1, 0].set_title('Polynomial Approximation')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].plot(x_fine, y_target, 'k-', linewidth=2, label='Target function')\n",
    "    axes[1, 1].plot(x_fine, fourier_approx, 'b--', linewidth=2, label=f'Fourier ({n_fourier} terms)')\n",
    "    axes[1, 1].plot(x_sample, y_sample, 'ro', markersize=4, label='Sample points')\n",
    "    axes[1, 1].set_xlabel('x')\n",
    "    axes[1, 1].set_ylabel('f(x)')\n",
    "    axes[1, 1].set_title('Fourier Approximation')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Compute approximation errors\n",
    "    poly_error = np.sqrt(np.mean((y_target - y_poly)**2))\n",
    "    fourier_error = np.sqrt(np.mean((y_target - fourier_approx)**2))\n",
    "    \n",
    "    print(f\"   Polynomial approximation L¬≤ error: {poly_error:.4f}\")\n",
    "    print(f\"   Fourier approximation L¬≤ error: {fourier_error:.4f}\")\n",
    "    \n",
    "    # 3. Completeness and convergence\n",
    "    print(\"\\n3. Completeness: Cauchy Sequences and Convergence\")\n",
    "    \n",
    "    # Demonstrate convergence in L¬≤ space\n",
    "    # Approximate a step function with smooth functions\n",
    "    x_step = np.linspace(-1, 1, 1000)\n",
    "    step_function = np.where(x_step > 0, 1, -1)\n",
    "    \n",
    "    # Smooth approximations with increasing steepness\n",
    "    steepness_values = [1, 2, 5, 10, 20, 50]\n",
    "    smooth_approximations = []\n",
    "    l2_errors = []\n",
    "    \n",
    "    for alpha in steepness_values:\n",
    "        smooth_approx = np.tanh(alpha * x_step)\n",
    "        smooth_approximations.append(smooth_approx)\n",
    "        \n",
    "        # L¬≤ error\n",
    "        error = np.sqrt(np.mean((step_function - smooth_approx)**2))\n",
    "        l2_errors.append(error)\n",
    "    \n",
    "    # Plot convergence\n",
    "    axes[1, 2].plot(x_step, step_function, 'k-', linewidth=3, label='Step function')\n",
    "    \n",
    "    colors_conv = plt.cm.plasma(np.linspace(0, 1, len(steepness_values)))\n",
    "    for i, (alpha, approx) in enumerate(zip(steepness_values, smooth_approximations)):\n",
    "        if i % 2 == 0:  # Plot every other approximation to avoid clutter\n",
    "            axes[1, 2].plot(x_step, approx, color=colors_conv[i], linewidth=2,\n",
    "                          alpha=0.7, label=f'Œ±={alpha}')\n",
    "    \n",
    "    axes[1, 2].set_xlabel('x')\n",
    "    axes[1, 2].set_ylabel('f(x)')\n",
    "    axes[1, 2].set_title('Convergence to Discontinuous Function')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"   L¬≤ convergence errors: {[f'{err:.4f}' for err in l2_errors]}\")\n",
    "    print(f\"   Sequence is Cauchy and converges in L¬≤ space\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return norms_df, poly_error, fourier_error\n",
    "\n",
    "norms_comparison, poly_err, fourier_err = demonstrate_function_spaces()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff88908",
   "metadata": {},
   "source": [
    "## 2. Reproducing Kernel Hilbert Spaces (RKHS)\n",
    "\n",
    "### The Heart of Kernel Methods\n",
    "A **Reproducing Kernel Hilbert Space** is a Hilbert space ‚Ñã of functions with a special property: point evaluation is a continuous linear functional.\n",
    "\n",
    "### The Reproducing Property\n",
    "For every x in the domain, there exists k(¬∑, x) ‚àà ‚Ñã such that:\n",
    "```\n",
    "f(x) = ‚ü®f, k(¬∑, x)‚ü©_‚Ñã  for all f ‚àà ‚Ñã\n",
    "```\n",
    "\n",
    "### Kernel Function\n",
    "The **kernel function** k(x, y) = ‚ü®k(¬∑, x), k(¬∑, y)‚ü©_‚Ñã satisfies:\n",
    "1. **Symmetry**: k(x, y) = k(y, x)\n",
    "2. **Positive definiteness**: For any x‚ÇÅ, ..., x‚Çô and c‚ÇÅ, ..., c‚Çô:\n",
    "   Œ£·µ¢‚±º c·µ¢c‚±ºk(x·µ¢, x‚±º) ‚â• 0\n",
    "\n",
    "### Common Kernels\n",
    "**RBF (Gaussian)**: k(x, y) = exp(-Œ≥‚Äñx - y‚Äñ¬≤)\n",
    "**Polynomial**: k(x, y) = (Œ≥‚ü®x, y‚ü© + r)^d\n",
    "**Linear**: k(x, y) = ‚ü®x, y‚ü©\n",
    "**Mat√©rn**: k(x, y) = (2^(1-ŒΩ)/Œì(ŒΩ))(‚àö(2ŒΩ)r)^ŒΩ K_ŒΩ(‚àö(2ŒΩ)r)\n",
    "\n",
    "### The Representer Theorem\n",
    "**Most important theorem in kernel methods**:\n",
    "The solution to the optimization problem:\n",
    "```\n",
    "min_f Œ£·µ¢ L(y·µ¢, f(x·µ¢)) + ŒªŒ©(‚Äñf‚Äñ_‚Ñã)\n",
    "```\n",
    "has the form f(x) = Œ£·µ¢ Œ±·µ¢k(x, x·µ¢)\n",
    "\n",
    "### Why RKHS Matters\n",
    "- **Feature maps**: k(x, y) = ‚ü®œÜ(x), œÜ(y)‚ü© (kernel trick)\n",
    "- **Infinite dimensions**: Work implicitly in high-dimensional spaces\n",
    "- **Regularization**: ‚Äñf‚Äñ_‚Ñã controls function complexity\n",
    "- **Universal approximation**: RBF kernels are universal approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_rkhs_and_kernels():\n",
    "    \"\"\"Explore Reproducing Kernel Hilbert Spaces and kernel methods\"\"\"\n",
    "    \n",
    "    print(\"üéØ RKHS and Kernel Methods: The Kernel Trick in Action\")\n",
    "    print(\"=\" * 56)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Kernel functions and their properties\n",
    "    print(\"\\n1. Kernel Functions: Building Blocks of RKHS\")\n",
    "    \n",
    "    # Define different kernel functions\n",
    "    def rbf_kernel(x, y, gamma=1.0):\n",
    "        return np.exp(-gamma * np.linalg.norm(x - y)**2)\n",
    "    \n",
    "    def polynomial_kernel(x, y, degree=3, gamma=1.0, coef0=1.0):\n",
    "        return (gamma * np.dot(x, y) + coef0)**degree\n",
    "    \n",
    "    def linear_kernel(x, y):\n",
    "        return np.dot(x, y)\n",
    "    \n",
    "    def matern_kernel(x, y, length_scale=1.0, nu=1.5):\n",
    "        r = np.linalg.norm(x - y) / length_scale\n",
    "        if nu == 0.5:\n",
    "            return np.exp(-r)\n",
    "        elif nu == 1.5:\n",
    "            return (1 + np.sqrt(3) * r) * np.exp(-np.sqrt(3) * r)\n",
    "        elif nu == 2.5:\n",
    "            return (1 + np.sqrt(5) * r + 5 * r**2 / 3) * np.exp(-np.sqrt(5) * r)\n",
    "        else:\n",
    "            # General case (simplified)\n",
    "            return np.exp(-r)\n",
    "    \n",
    "    # Visualize kernels as functions of distance\n",
    "    distances = np.linspace(0, 3, 100)\n",
    "    x_ref = np.array([0.0])\n",
    "    \n",
    "    kernel_values = {\n",
    "        'RBF (Œ≥=1)': [rbf_kernel(x_ref, np.array([d]), gamma=1.0) for d in distances],\n",
    "        'RBF (Œ≥=5)': [rbf_kernel(x_ref, np.array([d]), gamma=5.0) for d in distances],\n",
    "        'Mat√©rn (ŒΩ=0.5)': [matern_kernel(x_ref, np.array([d]), nu=0.5) for d in distances],\n",
    "        'Mat√©rn (ŒΩ=2.5)': [matern_kernel(x_ref, np.array([d]), nu=2.5) for d in distances],\n",
    "    }\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    for i, (name, values) in enumerate(kernel_values.items()):\n",
    "        axes[0, 0].plot(distances, values, color=colors[i], linewidth=2, label=name)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Distance |x - y|')\n",
    "    axes[0, 0].set_ylabel('Kernel Value k(x, y)')\n",
    "    axes[0, 0].set_title('Kernel Functions vs Distance')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"   Kernels measure similarity: k(x, x) = 1, k(x, y) ‚Üí 0 as |x-y| ‚Üí ‚àû\")\n",
    "    \n",
    "    # 2. Kernel matrix and positive definiteness\n",
    "    print(\"\\n2. Kernel Matrix: Positive Definiteness\")\n",
    "    \n",
    "    # Generate random points\n",
    "    np.random.seed(42)\n",
    "    n_points = 8\n",
    "    X_kernel = np.random.randn(n_points, 2)\n",
    "    \n",
    "    # Compute kernel matrices for different kernels\n",
    "    K_rbf = np.zeros((n_points, n_points))\n",
    "    K_poly = np.zeros((n_points, n_points))\n",
    "    K_linear = np.zeros((n_points, n_points))\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        for j in range(n_points):\n",
    "            K_rbf[i, j] = rbf_kernel(X_kernel[i], X_kernel[j], gamma=1.0)\n",
    "            K_poly[i, j] = polynomial_kernel(X_kernel[i], X_kernel[j], degree=2)\n",
    "            K_linear[i, j] = linear_kernel(X_kernel[i], X_kernel[j])\n",
    "    \n",
    "    # Check positive definiteness (all eigenvalues ‚â• 0)\n",
    "    eigvals_rbf = np.linalg.eigvals(K_rbf)\n",
    "    eigvals_poly = np.linalg.eigvals(K_poly)\n",
    "    eigvals_linear = np.linalg.eigvals(K_linear)\n",
    "    \n",
    "    # Visualize RBF kernel matrix\n",
    "    im = axes[0, 1].imshow(K_rbf, cmap='Blues', aspect='auto')\n",
    "    axes[0, 1].set_title('RBF Kernel Matrix')\n",
    "    axes[0, 1].set_xlabel('Data Point j')\n",
    "    axes[0, 1].set_ylabel('Data Point i')\n",
    "    plt.colorbar(im, ax=axes[0, 1], shrink=0.8)\n",
    "    \n",
    "    # Add kernel values\n",
    "    for i in range(n_points):\n",
    "        for j in range(n_points):\n",
    "            axes[0, 1].text(j, i, f'{K_rbf[i, j]:.2f}', ha='center', va='center',\n",
    "                           fontsize=6, color='white' if K_rbf[i, j] > 0.5 else 'black')\n",
    "    \n",
    "    print(f\"   RBF kernel eigenvalues (all ‚â• 0): min = {eigvals_rbf.min():.3f}, max = {eigvals_rbf.max():.3f}\")\n",
    "    print(f\"   Polynomial kernel eigenvalues: min = {eigvals_poly.min():.3f}, max = {eigvals_poly.max():.3f}\")\n",
    "    print(f\"   Linear kernel eigenvalues: min = {eigvals_linear.min():.3f}, max = {eigvals_linear.max():.3f}\")\n",
    "    \n",
    "    # 3. Kernel ridge regression\n",
    "    print(\"\\n3. Kernel Ridge Regression: Representer Theorem in Action\")\n",
    "    \n",
    "    # Generate 1D regression data\n",
    "    np.random.seed(42)\n",
    "    X_reg = np.linspace(0, 1, 20).reshape(-1, 1)\n",
    "    y_reg = np.sin(2 * np.pi * X_reg.flatten()) + 0.3 * np.random.randn(20)\n",
    "    \n",
    "    # Test points\n",
    "    X_test = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "    \n",
    "    # Different regularization parameters\n",
    "    alphas = [0.001, 0.1, 1.0]\n",
    "    colors_reg = ['blue', 'red', 'green']\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        # Kernel Ridge Regression\n",
    "        krr = KernelRidge(kernel='rbf', gamma=1.0, alpha=alpha)\n",
    "        krr.fit(X_reg, y_reg)\n",
    "        y_pred = krr.predict(X_test)\n",
    "        \n",
    "        axes[0, 2].plot(X_test.flatten(), y_pred, color=colors_reg[i], linewidth=2,\n",
    "                       label=f'Œ±={alpha}')\n",
    "    \n",
    "    # Plot true function and data\n",
    "    y_true = np.sin(2 * np.pi * X_test.flatten())\n",
    "    axes[0, 2].plot(X_test.flatten(), y_true, 'k--', linewidth=2, label='True function')\n",
    "    axes[0, 2].scatter(X_reg.flatten(), y_reg, color='black', s=50, zorder=5, label='Data')\n",
    "    \n",
    "    axes[0, 2].set_xlabel('x')\n",
    "    axes[0, 2].set_ylabel('y')\n",
    "    axes[0, 2].set_title('Kernel Ridge Regression')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"   Representer theorem: f(x) = Œ£·µ¢ Œ±·µ¢ k(x, x·µ¢)\")\n",
    "    print(f\"   Regularization Œ± controls smoothness\")\n",
    "    \n",
    "    # 4. Feature maps and the kernel trick\n",
    "    print(\"\\n4. Feature Maps: The Kernel Trick Revealed\")\n",
    "    \n",
    "    # Demonstrate feature map for polynomial kernel\n",
    "    # For degree 2 polynomial kernel in 2D: œÜ(x) = [x‚ÇÅ¬≤, ‚àö2x‚ÇÅx‚ÇÇ, x‚ÇÇ¬≤, ‚àö2x‚ÇÅ, ‚àö2x‚ÇÇ, 1]\n",
    "    \n",
    "    def polynomial_feature_map_2d(x, degree=2):\n",
    "        \"\"\"Explicit feature map for degree-2 polynomial kernel in 2D\"\"\"\n",
    "        x1, x2 = x[0], x[1]\n",
    "        if degree == 2:\n",
    "            return np.array([x1**2, np.sqrt(2)*x1*x2, x2**2, np.sqrt(2)*x1, np.sqrt(2)*x2, 1])\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    # Generate 2D data\n",
    "    np.random.seed(42)\n",
    "    X_2d = np.random.randn(5, 2)\n",
    "    \n",
    "    # Compute kernel matrix using explicit feature map\n",
    "    phi_X = np.array([polynomial_feature_map_2d(x) for x in X_2d])\n",
    "    K_explicit = phi_X @ phi_X.T\n",
    "    \n",
    "    # Compute kernel matrix using kernel function directly\n",
    "    K_direct = np.zeros((5, 5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            K_direct[i, j] = polynomial_kernel(X_2d[i], X_2d[j], degree=2, gamma=1.0, coef0=1.0)\n",
    "    \n",
    "    # Plot comparison\n",
    "    im1 = axes[1, 0].imshow(K_explicit, cmap='RdBu', aspect='auto')\n",
    "    axes[1, 0].set_title('Kernel via Feature Map')\n",
    "    axes[1, 0].set_xlabel('Data Point')\n",
    "    axes[1, 0].set_ylabel('Data Point')\n",
    "    \n",
    "    im2 = axes[1, 1].imshow(K_direct, cmap='RdBu', aspect='auto')\n",
    "    axes[1, 1].set_title('Kernel Function Direct')\n",
    "    axes[1, 1].set_xlabel('Data Point')\n",
    "    axes[1, 1].set_ylabel('Data Point')\n",
    "    \n",
    "    # Add values\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            axes[1, 0].text(j, i, f'{K_explicit[i, j]:.2f}', ha='center', va='center', fontsize=8)\n",
    "            axes[1, 1].text(j, i, f'{K_direct[i, j]:.2f}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im1, ax=axes[1, 0], shrink=0.8)\n",
    "    plt.colorbar(im2, ax=axes[1, 1], shrink=0.8)\n",
    "    \n",
    "    # Check if they're the same\n",
    "    kernel_difference = np.max(np.abs(K_explicit - K_direct))\n",
    "    print(f\"   Feature map dimension: {phi_X.shape[1]} (vs original 2D)\")\n",
    "    print(f\"   Kernel matrices difference: {kernel_difference:.10f}\")\n",
    "    print(f\"   Kernel trick: k(x, y) = ‚ü®œÜ(x), œÜ(y)‚ü© without computing œÜ explicitly\")\n",
    "    \n",
    "    # 5. Universal approximation with RBF kernels\n",
    "    print(\"\\n5. Universal Approximation: RBF Kernels Can Approximate Any Function\")\n",
    "    \n",
    "    # Complex target function\n",
    "    def complex_function(x):\n",
    "        return np.sin(5*x) * np.exp(-x) + 0.5*np.cos(10*x)\n",
    "    \n",
    "    X_approx = np.linspace(0, 2, 100).reshape(-1, 1)\n",
    "    y_target = complex_function(X_approx.flatten())\n",
    "    \n",
    "    # Different numbers of basis functions\n",
    "    n_basis_functions = [5, 10, 20]\n",
    "    colors_approx = ['blue', 'red', 'green']\n",
    "    \n",
    "    for i, n_basis in enumerate(n_basis_functions):\n",
    "        # Select basis points\n",
    "        X_basis = np.linspace(0, 2, n_basis).reshape(-1, 1)\n",
    "        y_basis = complex_function(X_basis.flatten())\n",
    "        \n",
    "        # Kernel ridge regression with RBF kernel\n",
    "        krr_approx = KernelRidge(kernel='rbf', gamma=5.0, alpha=0.01)\n",
    "        krr_approx.fit(X_basis, y_basis)\n",
    "        y_approx = krr_approx.predict(X_approx)\n",
    "        \n",
    "        axes[1, 2].plot(X_approx.flatten(), y_approx, color=colors_approx[i],\n",
    "                       linewidth=2, label=f'{n_basis} basis functions')\n",
    "    \n",
    "    axes[1, 2].plot(X_approx.flatten(), y_target, 'k--', linewidth=2, label='Target function')\n",
    "    axes[1, 2].set_xlabel('x')\n",
    "    axes[1, 2].set_ylabel('f(x)')\n",
    "    axes[1, 2].set_title('Universal Approximation with RBF')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Compute approximation errors\n",
    "    for i, n_basis in enumerate(n_basis_functions):\n",
    "        X_basis = np.linspace(0, 2, n_basis).reshape(-1, 1)\n",
    "        y_basis = complex_function(X_basis.flatten())\n",
    "        krr_approx = KernelRidge(kernel='rbf', gamma=5.0, alpha=0.01)\n",
    "        krr_approx.fit(X_basis, y_basis)\n",
    "        y_approx = krr_approx.predict(X_approx)\n",
    "        error = np.sqrt(np.mean((y_target - y_approx)**2))\n",
    "        print(f\"   {n_basis} basis functions: RMSE = {error:.4f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return K_rbf, eigvals_rbf, kernel_difference\n",
    "\n",
    "K_demo, eigvals_demo, kernel_diff = demonstrate_rkhs_and_kernels()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
