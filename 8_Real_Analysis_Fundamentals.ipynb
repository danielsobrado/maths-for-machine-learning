{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87dd6fb9",
   "metadata": {},
   "source": [
    "# Real Analysis Fundamentals for Machine Learning\n",
    "## Convergence Theory and Function Approximation\n",
    "\n",
    "Welcome to the **theoretical foundation** that guarantees machine learning actually works! Real analysis provides the mathematical rigor behind why neural networks converge and how well they can approximate functions.\n",
    "\n",
    "### What You'll Master\n",
    "By the end of this notebook, you'll understand:\n",
    "1. **Convergence theory** - When and why algorithms converge to solutions\n",
    "2. **Function approximation** - How neural networks approximate any function\n",
    "3. **Continuity and limits** - The foundation of optimization theory\n",
    "4. **Uniform convergence** - When approximations work globally\n",
    "5. **Metric spaces** - The mathematical framework for learning\n",
    "6. **Compactness** - Why some problems are easier than others\n",
    "\n",
    "### Why This Matters\n",
    "- **Convergence guarantees** tell us if our algorithms will work\n",
    "- **Approximation theory** explains why neural networks are so powerful\n",
    "- **Continuity** ensures small changes in input don't break everything\n",
    "- **Metric spaces** provide the foundation for similarity and distance\n",
    "\n",
    "### Real-World Impact\n",
    "- **Universal approximation theorem**: Neural networks can approximate any function\n",
    "- **Gradient descent convergence**: Conditions under which training succeeds\n",
    "- **Generalization bounds**: How well models perform on new data\n",
    "\n",
    "Let's dive into the mathematical foundations! ðŸ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ffc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import optimize\n",
    "from scipy.interpolate import interp1d, BSpline, splrep, splev\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define some mathematical constants\n",
    "GOLDEN_RATIO = (1 + np.sqrt(5)) / 2\n",
    "EULER_CONSTANT = np.e\n",
    "\n",
    "print(\"ðŸ“ Real Analysis toolkit loaded!\")\n",
    "print(\"Ready to explore the foundations of mathematics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b53515",
   "metadata": {},
   "source": [
    "## 1. Sequences and Convergence: The Foundation\n",
    "\n",
    "### What is Convergence?\n",
    "A sequence `{aâ‚™}` **converges** to limit `L` if:\n",
    "```\n",
    "For every Îµ > 0, there exists N such that\n",
    "for all n > N: |aâ‚™ - L| < Îµ\n",
    "```\n",
    "\n",
    "**Translation**: No matter how close you want to get to L (Îµ), you can always find a point in the sequence after which all terms stay within that distance.\n",
    "\n",
    "### Types of Convergence\n",
    "\n",
    "#### 1. Pointwise Convergence\n",
    "Functions `fâ‚™(x)` converge to `f(x)` at each point x individually.\n",
    "\n",
    "#### 2. Uniform Convergence\n",
    "Functions `fâ‚™(x)` converge to `f(x)` **simultaneously** for all x in the domain.\n",
    "```\n",
    "For every Îµ > 0, there exists N such that\n",
    "for all n > N and all x: |fâ‚™(x) - f(x)| < Îµ\n",
    "```\n",
    "\n",
    "#### 3. LÂ² Convergence (Mean Square)\n",
    "```\n",
    "âˆ« |fâ‚™(x) - f(x)|Â² dx â†’ 0 as n â†’ âˆž\n",
    "```\n",
    "\n",
    "### Why This Matters in ML\n",
    "- **Gradient descent**: Does the sequence of parameters converge?\n",
    "- **Neural network training**: Do the weights converge to optimal values?\n",
    "- **Function approximation**: How fast do approximations improve?\n",
    "- **Generalization**: Does performance on training set predict test performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63990a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_convergence_types():\n",
    "    \"\"\"Visualize different types of convergence\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ Types of Convergence Demonstration\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Domain\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Pointwise convergence (but not uniform)\n",
    "    print(\"\\n1. Pointwise Convergence (Non-Uniform)\")\n",
    "    print(\"   fâ‚™(x) = x^n on [0,1]\")\n",
    "    print(\"   Limit: f(x) = 0 for x âˆˆ [0,1), f(1) = 1\")\n",
    "    \n",
    "    ax1 = axes[0, 0]\n",
    "    for n in [1, 2, 5, 10, 20, 50]:\n",
    "        fn = x**n\n",
    "        ax1.plot(x, fn, alpha=0.7, label=f'n={n}')\n",
    "    \n",
    "    # Limit function\n",
    "    limit_pointwise = np.zeros_like(x)\n",
    "    limit_pointwise[-1] = 1  # Jump at x=1\n",
    "    ax1.plot(x, limit_pointwise, 'k--', linewidth=3, label='Limit')\n",
    "    ax1.set_title('Pointwise Convergence\\n$f_n(x) = x^n$')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('$f_n(x)$')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Uniform convergence\n",
    "    print(\"\\n2. Uniform Convergence\")\n",
    "    print(\"   fâ‚™(x) = x/n on [0,1]\")\n",
    "    print(\"   Limit: f(x) = 0 uniformly\")\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    for n in [1, 2, 5, 10, 20, 50]:\n",
    "        fn = x / n\n",
    "        ax2.plot(x, fn, alpha=0.7, label=f'n={n}')\n",
    "    \n",
    "    ax2.axhline(y=0, color='k', linestyle='--', linewidth=3, label='Limit')\n",
    "    ax2.set_title('Uniform Convergence\\n$f_n(x) = x/n$')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.set_ylabel('$f_n(x)$')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Convergence in LÂ² norm\n",
    "    print(\"\\n3. LÂ² Convergence\")\n",
    "    print(\"   fâ‚™(x) = sin(nx)/âˆšn\")\n",
    "    print(\"   ||fâ‚™||â‚‚ â†’ 0 but pointwise limit doesn't exist\")\n",
    "    \n",
    "    ax3 = axes[0, 2]\n",
    "    l2_norms = []\n",
    "    for n in [1, 2, 5, 10, 20, 50]:\n",
    "        fn = np.sin(n * x) / np.sqrt(n)\n",
    "        l2_norm = np.sqrt(np.trapz(fn**2, x))\n",
    "        l2_norms.append(l2_norm)\n",
    "        if n <= 10:\n",
    "            ax3.plot(x, fn, alpha=0.7, label=f'n={n}')\n",
    "    \n",
    "    ax3.set_title('LÂ² Convergence\\n$f_n(x) = \\\\sin(nx)/\\\\sqrt{n}$')\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('$f_n(x)$')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Gradient descent convergence\n",
    "    print(\"\\n4. Gradient Descent Convergence\")\n",
    "    print(\"   Minimizing f(x) = xÂ² starting from xâ‚€ = 5\")\n",
    "    \n",
    "    def gradient_descent_demo(x0, learning_rate, num_steps):\n",
    "        x = x0\n",
    "        path = [x]\n",
    "        for _ in range(num_steps):\n",
    "            gradient = 2 * x  # df/dx for f(x) = xÂ²\n",
    "            x = x - learning_rate * gradient\n",
    "            path.append(x)\n",
    "        return np.array(path)\n",
    "    \n",
    "    ax4 = axes[1, 0]\n",
    "    learning_rates = [0.1, 0.3, 0.5, 0.9, 1.1]\n",
    "    for lr in learning_rates:\n",
    "        path = gradient_descent_demo(5.0, lr, 20)\n",
    "        iterations = range(len(path))\n",
    "        convergent = lr < 1.0\n",
    "        style = '-' if convergent else '--'\n",
    "        ax4.plot(iterations, path, style, alpha=0.8, label=f'lr={lr}')\n",
    "    \n",
    "    ax4.axhline(y=0, color='k', linestyle=':', label='Optimum')\n",
    "    ax4.set_title('Gradient Descent Convergence')\n",
    "    ax4.set_xlabel('Iteration')\n",
    "    ax4.set_ylabel('x value')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_yscale('symlog')\n",
    "    \n",
    "    # 5. Neural network approximation convergence\n",
    "    print(\"\\n5. Neural Network Function Approximation\")\n",
    "    print(\"   Approximating f(x) = sin(2Ï€x) + 0.3*sin(6Ï€x)\")\n",
    "    \n",
    "    def target_function(x):\n",
    "        return np.sin(2 * np.pi * x) + 0.3 * np.sin(6 * np.pi * x)\n",
    "    \n",
    "    x_train = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "    y_train = target_function(x_train.flatten())\n",
    "    x_test = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "    y_true = target_function(x_test.flatten())\n",
    "    \n",
    "    ax5 = axes[1, 1]\n",
    "    ax5.plot(x_test, y_true, 'k-', linewidth=3, label='True function')\n",
    "    \n",
    "    hidden_sizes = [5, 10, 20, 50]\n",
    "    for size in hidden_sizes:\n",
    "        mlp = MLPRegressor(hidden_layer_sizes=(size,), max_iter=1000, random_state=42)\n",
    "        mlp.fit(x_train, y_train)\n",
    "        y_pred = mlp.predict(x_test)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        ax5.plot(x_test, y_pred, alpha=0.7, label=f'{size} neurons (MSE={mse:.3f})')\n",
    "    \n",
    "    ax5.set_title('Neural Network Approximation')\n",
    "    ax5.set_xlabel('x')\n",
    "    ax5.set_ylabel('f(x)')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Convergence rates\n",
    "    print(\"\\n6. Convergence Rate Comparison\")\n",
    "    \n",
    "    n_values = np.arange(1, 51)\n",
    "    \n",
    "    # Different convergence rates\n",
    "    linear = 1 / n_values\n",
    "    quadratic = 1 / n_values**2\n",
    "    exponential = np.exp(-n_values)\n",
    "    geometric = 0.9**n_values\n",
    "    \n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.semilogy(n_values, linear, 'o-', label='Linear: 1/n', alpha=0.8)\n",
    "    ax6.semilogy(n_values, quadratic, 's-', label='Quadratic: 1/nÂ²', alpha=0.8)\n",
    "    ax6.semilogy(n_values, exponential, '^-', label='Exponential: $e^{-n}$', alpha=0.8)\n",
    "    ax6.semilogy(n_values, geometric, 'd-', label='Geometric: $0.9^n$', alpha=0.8)\n",
    "    \n",
    "    ax6.set_title('Convergence Rates')\n",
    "    ax6.set_xlabel('n (iteration/degree)')\n",
    "    ax6.set_ylabel('Error (log scale)')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“Š Key Insights:\")\n",
    "    print(\"â€¢ Pointwise â‰  Uniform: Functions can converge at each point but not uniformly\")\n",
    "    print(\"â€¢ Learning rate matters: Too large â†’ divergence, too small â†’ slow convergence\")\n",
    "    print(\"â€¢ More neurons â†’ better approximation (with enough data)\")\n",
    "    print(\"â€¢ Exponential convergence is much faster than polynomial\")\n",
    "\n",
    "demonstrate_convergence_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f3aa14",
   "metadata": {},
   "source": [
    "## 2. Continuity and Differentiability: Smoothness Matters\n",
    "\n",
    "### Continuity\n",
    "A function f is **continuous** at point a if:\n",
    "```\n",
    "lim(xâ†’a) f(x) = f(a)\n",
    "```\n",
    "\n",
    "**Three conditions must hold**:\n",
    "1. `f(a)` exists (function is defined at a)\n",
    "2. `lim(xâ†’a) f(x)` exists (limit exists)\n",
    "3. The limit equals the function value\n",
    "\n",
    "### Types of Continuity\n",
    "\n",
    "#### Uniform Continuity\n",
    "For every Îµ > 0, there exists Î´ > 0 such that for **all** x, y:\n",
    "```\n",
    "|x - y| < Î´  âŸ¹  |f(x) - f(y)| < Îµ\n",
    "```\n",
    "\n",
    "#### Lipschitz Continuity\n",
    "There exists L > 0 such that for all x, y:\n",
    "```\n",
    "|f(x) - f(y)| â‰¤ L|x - y|\n",
    "```\n",
    "\n",
    "### Why Continuity Matters in ML\n",
    "\n",
    "1. **Optimization**: Continuous functions are easier to optimize\n",
    "2. **Generalization**: Small changes in input should give small changes in output\n",
    "3. **Robustness**: Continuous models are less sensitive to noise\n",
    "4. **Convergence**: Many convergence theorems require continuity\n",
    "\n",
    "### Differentiability Hierarchy\n",
    "```\n",
    "Lipschitz âŸ¹ Uniformly Continuous âŸ¹ Continuous\n",
    "                     â‡…\n",
    "            Differentiable âŸ¹ Continuous\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_continuity_and_differentiability():\n",
    "    \"\"\"Explore different types of continuity and their ML implications\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Continuity and Differentiability in ML\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    x = np.linspace(-2, 2, 1000)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Discontinuous function\n",
    "    print(\"\\n1. Discontinuous Function (Step Function)\")\n",
    "    print(\"   Problem: Gradient doesn't exist at discontinuities\")\n",
    "    \n",
    "    step_func = np.where(x < 0, -1, 1)\n",
    "    axes[0, 0].plot(x, step_func, 'b-', linewidth=2)\n",
    "    axes[0, 0].scatter([0], [1], color='red', s=100, zorder=5)\n",
    "    axes[0, 0].scatter([0], [-1], color='red', s=100, zorder=5, facecolors='none', edgecolors='red')\n",
    "    axes[0, 0].set_title('Discontinuous Function\\n(Step Function)')\n",
    "    axes[0, 0].set_xlabel('x')\n",
    "    axes[0, 0].set_ylabel('f(x)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].text(0.5, 0, 'No gradient here!', fontsize=10, color='red')\n",
    "    \n",
    "    # 2. Continuous but not differentiable\n",
    "    print(\"\\n2. Continuous but Not Differentiable (ReLU)\")\n",
    "    print(\"   Used in: Neural networks (despite non-differentiability at 0)\")\n",
    "    \n",
    "    relu = np.maximum(0, x)\n",
    "    axes[0, 1].plot(x, relu, 'g-', linewidth=2)\n",
    "    axes[0, 1].scatter([0], [0], color='red', s=100, zorder=5)\n",
    "    axes[0, 1].set_title('ReLU Activation\\n$f(x) = \\\\max(0, x)$')\n",
    "    axes[0, 1].set_xlabel('x')\n",
    "    axes[0, 1].set_ylabel('f(x)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].text(0.2, 0.2, 'Gradient undefined\\nat x=0', fontsize=10, color='red')\n",
    "    \n",
    "    # 3. Smooth and differentiable\n",
    "    print(\"\\n3. Smooth and Differentiable (Sigmoid)\")\n",
    "    print(\"   Used in: Classical neural networks, logistic regression\")\n",
    "    \n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    axes[0, 2].plot(x, sigmoid, 'purple', linewidth=2)\n",
    "    axes[0, 2].set_title('Sigmoid Activation\\n$f(x) = \\\\frac{1}{1+e^{-x}}$')\n",
    "    axes[0, 2].set_xlabel('x')\n",
    "    axes[0, 2].set_ylabel('f(x)')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].text(0, 0.2, 'Smooth everywhere', fontsize=10, color='green')\n",
    "    \n",
    "    # 4. Lipschitz continuous function\n",
    "    print(\"\\n4. Lipschitz Continuous Function\")\n",
    "    print(\"   Property: |f(x) - f(y)| â‰¤ L|x - y| for some L\")\n",
    "    \n",
    "    # Example: f(x) = tanh(x) is 1-Lipschitz\n",
    "    tanh_func = np.tanh(x)\n",
    "    axes[1, 0].plot(x, tanh_func, 'orange', linewidth=2, label='tanh(x)')\n",
    "    \n",
    "    # Show Lipschitz property\n",
    "    x1, x2 = 0.5, 1.0\n",
    "    y1, y2 = np.tanh(x1), np.tanh(x2)\n",
    "    axes[1, 0].plot([x1, x2], [y1, y2], 'ro-', markersize=8)\n",
    "    axes[1, 0].plot([x1, x2], [y1, y1 + 1*(x2-x1)], 'r--', alpha=0.7, label='L=1 bound')\n",
    "    axes[1, 0].set_title('Lipschitz Continuous\\n$f(x) = \\\\tanh(x)$')\n",
    "    axes[1, 0].set_xlabel('x')\n",
    "    axes[1, 0].set_ylabel('f(x)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Impact on optimization\n",
    "    print(\"\\n5. Impact on Gradient Descent Convergence\")\n",
    "    print(\"   Smooth functions â†’ guaranteed convergence\")\n",
    "    print(\"   Non-smooth functions â†’ may not converge\")\n",
    "    \n",
    "    # Optimize smooth vs non-smooth functions\n",
    "    def smooth_objective(x):\n",
    "        return x**2 + 0.1 * x**4\n",
    "    \n",
    "    def nonsmooth_objective(x):\n",
    "        return abs(x) + 0.1 * x**2\n",
    "    \n",
    "    x_opt = np.linspace(-2, 2, 1000)\n",
    "    y_smooth = [smooth_objective(xi) for xi in x_opt]\n",
    "    y_nonsmooth = [nonsmooth_objective(xi) for xi in x_opt]\n",
    "    \n",
    "    axes[1, 1].plot(x_opt, y_smooth, 'b-', linewidth=2, label='Smooth: $x^2 + 0.1x^4$')\n",
    "    axes[1, 1].plot(x_opt, y_nonsmooth, 'r-', linewidth=2, label='Non-smooth: $|x| + 0.1x^2$')\n",
    "    axes[1, 1].set_title('Optimization Landscapes')\n",
    "    axes[1, 1].set_xlabel('x')\n",
    "    axes[1, 1].set_ylabel('f(x)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Regularization and smoothness\n",
    "    print(\"\\n6. Regularization Promotes Smoothness\")\n",
    "    print(\"   L2 regularization makes functions smoother\")\n",
    "    \n",
    "    # Generate noisy data\n",
    "    np.random.seed(42)\n",
    "    x_data = np.linspace(0, 1, 20)\n",
    "    y_data = np.sin(2 * np.pi * x_data) + 0.2 * np.random.randn(20)\n",
    "    \n",
    "    x_smooth = np.linspace(0, 1, 200)\n",
    "    \n",
    "    # Polynomial fit without regularization (overfitting)\n",
    "    coeffs_overfit = np.polyfit(x_data, y_data, 10)\n",
    "    y_overfit = np.polyval(coeffs_overfit, x_smooth)\n",
    "    \n",
    "    # Polynomial fit with L2 regularization (Ridge regression)\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    poly_features = PolynomialFeatures(degree=10)\n",
    "    X_poly = poly_features.fit_transform(x_data.reshape(-1, 1))\n",
    "    X_smooth_poly = poly_features.transform(x_smooth.reshape(-1, 1))\n",
    "    \n",
    "    ridge = Ridge(alpha=0.1)\n",
    "    ridge.fit(X_poly, y_data)\n",
    "    y_regularized = ridge.predict(X_smooth_poly)\n",
    "    \n",
    "    axes[1, 2].scatter(x_data, y_data, color='black', s=50, zorder=5, label='Data')\n",
    "    axes[1, 2].plot(x_smooth, y_overfit, 'r-', linewidth=2, alpha=0.7, label='Overfit (non-smooth)')\n",
    "    axes[1, 2].plot(x_smooth, y_regularized, 'b-', linewidth=2, label='Regularized (smooth)')\n",
    "    axes[1, 2].set_title('Regularization & Smoothness')\n",
    "    axes[1, 2].set_xlabel('x')\n",
    "    axes[1, 2].set_ylabel('y')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ Key Takeaways:\")\n",
    "    print(\"â€¢ Discontinuities break gradient-based optimization\")\n",
    "    print(\"â€¢ ReLU works despite non-differentiability (subgradients)\")\n",
    "    print(\"â€¢ Smooth functions are easier to optimize\")\n",
    "    print(\"â€¢ Lipschitz continuity guarantees bounded sensitivity\")\n",
    "    print(\"â€¢ Regularization promotes smoothness and generalization\")\n",
    "\n",
    "explore_continuity_and_differentiability()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
