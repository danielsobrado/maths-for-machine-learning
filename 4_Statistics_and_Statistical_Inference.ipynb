{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics and Statistical Inference for Machine Learning\n",
    "\n",
    "This notebook covers essential statistical concepts for machine learning:\n",
    "- Descriptive statistics and data exploration\n",
    "- Hypothesis testing and p-values\n",
    "- Confidence intervals and statistical inference\n",
    "- Model evaluation and validation\n",
    "- A/B testing and experimental design\n",
    "- Bias, variance, and the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import cross_val_score, learning_curve, validation_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descriptive Statistics and Data Exploration\n",
    "\n",
    "Understanding your data is the first step in any ML project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create mixed distributions to demonstrate different statistical properties\n",
    "data = {\n",
    "    'normal': np.random.normal(100, 15, n_samples),\n",
    "    'skewed': np.random.exponential(2, n_samples),\n",
    "    'bimodal': np.concatenate([np.random.normal(80, 10, n_samples//2),\n",
    "                              np.random.normal(120, 8, n_samples//2)]),\n",
    "    'uniform': np.random.uniform(50, 150, n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate descriptive statistics\n",
    "def calculate_stats(data):\n",
    "    \"\"\"Calculate comprehensive descriptive statistics\"\"\"\n",
    "    stats_dict = {\n",
    "        'mean': np.mean(data),\n",
    "        'median': np.median(data),\n",
    "        'mode': stats.mode(data, keepdims=True)[0][0],\n",
    "        'std': np.std(data, ddof=1),\n",
    "        'variance': np.var(data, ddof=1),\n",
    "        'skewness': stats.skew(data),\n",
    "        'kurtosis': stats.kurtosis(data),\n",
    "        'min': np.min(data),\n",
    "        'max': np.max(data),\n",
    "        'q25': np.percentile(data, 25),\n",
    "        'q75': np.percentile(data, 75),\n",
    "        'iqr': np.percentile(data, 75) - np.percentile(data, 25)\n",
    "    }\n",
    "    return stats_dict\n",
    "\n",
    "# Visualize distributions and statistics\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 12))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'purple']\n",
    "\n",
    "for i, (name, values) in enumerate(data.items()):\n",
    "    stats_dict = calculate_stats(values)\n",
    "    \n",
    "    # Histogram with statistics\n",
    "    axes[0, i].hist(values, bins=50, alpha=0.7, color=colors[i], density=True, edgecolor='black')\n",
    "    axes[0, i].axvline(stats_dict['mean'], color='red', linestyle='-', linewidth=2, label=f'Mean: {stats_dict[\"mean\"]:.1f}')\n",
    "    axes[0, i].axvline(stats_dict['median'], color='green', linestyle='--', linewidth=2, label=f'Median: {stats_dict[\"median\"]:.1f}')\n",
    "    \n",
    "    axes[0, i].set_title(f'{name.title()} Distribution', fontweight='bold')\n",
    "    axes[0, i].set_xlabel('Value')\n",
    "    axes[0, i].set_ylabel('Density')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    bp = axes[1, i].boxplot(values, patch_artist=True, widths=0.6)\n",
    "    bp['boxes'][0].set_facecolor(colors[i])\n",
    "    bp['boxes'][0].set_alpha(0.7)\n",
    "    \n",
    "    axes[1, i].set_title(f'{name.title()}: Skew={stats_dict[\"skewness\"]:.2f}', fontweight='bold')\n",
    "    axes[1, i].set_ylabel('Value')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create summary statistics table\n",
    "stats_df = pd.DataFrame({name: calculate_stats(values) for name, values in data.items()}).T\n",
    "print(\"Descriptive Statistics Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(stats_df.round(2))\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nInterpretations:\")\n",
    "print(\"- Normal: Symmetric, skewness ≈ 0, mean ≈ median\")\n",
    "print(\"- Skewed: Right-tailed, positive skewness, mean > median\")\n",
    "print(\"- Bimodal: Two peaks, may have negative kurtosis\")\n",
    "print(\"- Uniform: Flat distribution, negative kurtosis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hypothesis Testing\n",
    "\n",
    "Statistical tests to make decisions about populations from sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing examples\n",
    "\n",
    "# 1. One-sample t-test\n",
    "def one_sample_ttest_demo():\n",
    "    \"\"\"Demonstrate one-sample t-test\"\"\"\n",
    "    # H0: population mean = 100\n",
    "    # H1: population mean ≠ 100\n",
    "    \n",
    "    sample_data = np.random.normal(105, 15, 50)  # True mean = 105\n",
    "    hypothesized_mean = 100\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_1samp(sample_data, hypothesized_mean)\n",
    "    \n",
    "    print(\"One-Sample T-Test\")\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Sample mean: {np.mean(sample_data):.2f}\")\n",
    "    print(f\"Hypothesized mean: {hypothesized_mean}\")\n",
    "    print(f\"T-statistic: {t_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    print(f\"Conclusion: {'Reject H0' if p_value < 0.05 else 'Fail to reject H0'} (α=0.05)\")\n",
    "    \n",
    "    return sample_data, t_stat, p_value\n",
    "\n",
    "# 2. Two-sample t-test\n",
    "def two_sample_ttest_demo():\n",
    "    \"\"\"Demonstrate two-sample t-test\"\"\"\n",
    "    # H0: mean1 = mean2\n",
    "    # H1: mean1 ≠ mean2\n",
    "    \n",
    "    group1 = np.random.normal(100, 15, 50)\n",
    "    group2 = np.random.normal(108, 15, 45)  # Different mean\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "    \n",
    "    print(\"\\nTwo-Sample T-Test\")\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Group 1 mean: {np.mean(group1):.2f}\")\n",
    "    print(f\"Group 2 mean: {np.mean(group2):.2f}\")\n",
    "    print(f\"T-statistic: {t_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    print(f\"Conclusion: {'Reject H0' if p_value < 0.05 else 'Fail to reject H0'} (α=0.05)\")\n",
    "    \n",
    "    return group1, group2, t_stat, p_value\n",
    "\n",
    "# 3. Chi-square test\n",
    "def chi_square_test_demo():\n",
    "    \"\"\"Demonstrate chi-square test for independence\"\"\"\n",
    "    # Test independence between two categorical variables\n",
    "    observed = np.array([[20, 30, 15],   # Group A\n",
    "                        [25, 20, 10],   # Group B\n",
    "                        [15, 25, 20]])  # Group C\n",
    "    \n",
    "    chi2_stat, p_value, dof, expected = stats.chi2_contingency(observed)\n",
    "    \n",
    "    print(\"\\nChi-Square Test of Independence\")\n",
    "    print(\"=\" * 35)\n",
    "    print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    print(f\"Degrees of freedom: {dof}\")\n",
    "    print(f\"Conclusion: {'Variables are dependent' if p_value < 0.05 else 'Variables are independent'} (α=0.05)\")\n",
    "    \n",
    "    return observed, expected, chi2_stat, p_value\n",
    "\n",
    "# Run demonstrations\n",
    "sample1, t1, p1 = one_sample_ttest_demo()\n",
    "group1, group2, t2, p2 = two_sample_ttest_demo()\n",
    "observed, expected, chi2, p_chi = chi_square_test_demo()\n",
    "\n",
    "# Visualize hypothesis tests\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# One-sample t-test visualization\n",
    "x = np.linspace(np.min(sample1), np.max(sample1), 100)\n",
    "axes[0, 0].hist(sample1, bins=15, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(np.mean(sample1), color='red', linestyle='-', linewidth=2, label=f'Sample mean: {np.mean(sample1):.2f}')\n",
    "axes[0, 0].axvline(100, color='green', linestyle='--', linewidth=2, label='H0 mean: 100')\n",
    "axes[0, 0].set_title(f'One-Sample T-Test (p={p1:.4f})', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Two-sample t-test visualization\n",
    "axes[0, 1].hist(group1, bins=15, alpha=0.6, density=True, color='blue', label=f'Group 1 (μ={np.mean(group1):.1f})', edgecolor='black')\n",
    "axes[0, 1].hist(group2, bins=15, alpha=0.6, density=True, color='red', label=f'Group 2 (μ={np.mean(group2):.1f})', edgecolor='black')\n",
    "axes[0, 1].set_title(f'Two-Sample T-Test (p={p2:.4f})', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Value')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# P-value distribution\n",
    "# Simulate multiple tests to show p-value distribution under null hypothesis\n",
    "n_simulations = 1000\n",
    "p_values_null = []\n",
    "for _ in range(n_simulations):\n",
    "    null_sample = np.random.normal(100, 15, 50)  # True null hypothesis\n",
    "    _, p_val = stats.ttest_1samp(null_sample, 100)\n",
    "    p_values_null.append(p_val)\n",
    "\n",
    "axes[1, 0].hist(p_values_null, bins=20, alpha=0.7, density=True, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].axhline(y=1, color='red', linestyle='--', linewidth=2, label='Uniform under H0')\n",
    "axes[1, 0].axvline(0.05, color='red', linestyle='-', linewidth=2, label='α = 0.05')\n",
    "axes[1, 0].set_title('P-value Distribution Under H0', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('P-value')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Type I and Type II errors\n",
    "effect_sizes = np.linspace(0, 10, 100)\n",
    "power_values = []\n",
    "\n",
    "for effect in effect_sizes:\n",
    "    # Calculate power for given effect size\n",
    "    power = stats.ttest_power(effect/15, 50, 0.05)  # effect/std, sample_size, alpha\n",
    "    power_values.append(power)\n",
    "\n",
    "axes[1, 1].plot(effect_sizes, power_values, 'b-', linewidth=3, label='Statistical Power')\n",
    "axes[1, 1].axhline(y=0.8, color='red', linestyle='--', linewidth=2, label='Power = 0.8')\n",
    "axes[1, 1].axhline(y=0.05, color='orange', linestyle='--', linewidth=2, label='Type I Error = 0.05')\n",
    "axes[1, 1].set_title('Statistical Power vs Effect Size', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Effect Size')\n",
    "axes[1, 1].set_ylabel('Power (1 - Type II Error)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confidence Intervals\n",
    "\n",
    "Quantifying uncertainty in statistical estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence interval demonstrations\n",
    "\n",
    "def simulate_confidence_intervals(true_mean=100, true_std=15, sample_size=50, \n",
    "                                confidence_level=0.95, n_simulations=100):\n",
    "    \"\"\"Simulate confidence intervals to show coverage probability\"\"\"\n",
    "    \n",
    "    alpha = 1 - confidence_level\n",
    "    t_critical = stats.t.ppf(1 - alpha/2, sample_size - 1)\n",
    "    \n",
    "    intervals = []\n",
    "    sample_means = []\n",
    "    contains_true_mean = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate sample\n",
    "        sample = np.random.normal(true_mean, true_std, sample_size)\n",
    "        sample_mean = np.mean(sample)\n",
    "        sample_std = np.std(sample, ddof=1)\n",
    "        \n",
    "        # Calculate confidence interval\n",
    "        margin_error = t_critical * (sample_std / np.sqrt(sample_size))\n",
    "        ci_lower = sample_mean - margin_error\n",
    "        ci_upper = sample_mean + margin_error\n",
    "        \n",
    "        intervals.append((ci_lower, ci_upper))\n",
    "        sample_means.append(sample_mean)\n",
    "        contains_true_mean.append(ci_lower <= true_mean <= ci_upper)\n",
    "    \n",
    "    return intervals, sample_means, contains_true_mean\n",
    "\n",
    "# Run simulation\n",
    "intervals, means, coverage = simulate_confidence_intervals(n_simulations=100)\n",
    "coverage_rate = np.mean(coverage)\n",
    "\n",
    "print(f\"Confidence Interval Simulation Results:\")\n",
    "print(f\"True coverage rate: {coverage_rate:.1%} (Expected: 95%)\")\n",
    "print(f\"Number of intervals containing true mean: {sum(coverage)}/100\")\n",
    "\n",
    "# Visualize confidence intervals\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot first 50 confidence intervals\n",
    "n_show = 50\n",
    "for i in range(n_show):\n",
    "    color = 'green' if coverage[i] else 'red'\n",
    "    ax1.plot([intervals[i][0], intervals[i][1]], [i, i], color=color, linewidth=2, alpha=0.7)\n",
    "    ax1.plot(means[i], i, 'o', color=color, markersize=4)\n",
    "\n",
    "ax1.axvline(100, color='blue', linestyle='--', linewidth=3, label='True mean')\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Sample Number')\n",
    "ax1.set_title(f'95% Confidence Intervals (First {n_show} samples)\\nGreen: Contains true mean, Red: Misses true mean', \n",
    "             fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bootstrap confidence intervals\n",
    "def bootstrap_ci(data, n_bootstrap=1000, confidence_level=0.95):\n",
    "    \"\"\"Calculate bootstrap confidence interval\"\"\"\n",
    "    bootstrap_means = []\n",
    "    n = len(data)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_sample = np.random.choice(data, size=n, replace=True)\n",
    "        bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "    \n",
    "    alpha = 1 - confidence_level\n",
    "    ci_lower = np.percentile(bootstrap_means, 100 * alpha/2)\n",
    "    ci_upper = np.percentile(bootstrap_means, 100 * (1 - alpha/2))\n",
    "    \n",
    "    return bootstrap_means, ci_lower, ci_upper\n",
    "\n",
    "# Compare different CI methods\n",
    "sample_data = np.random.normal(100, 15, 100)\n",
    "sample_mean = np.mean(sample_data)\n",
    "sample_std = np.std(sample_data, ddof=1)\n",
    "\n",
    "# T-distribution CI\n",
    "t_crit = stats.t.ppf(0.975, len(sample_data) - 1)\n",
    "t_ci = (sample_mean - t_crit * sample_std/np.sqrt(len(sample_data)),\n",
    "        sample_mean + t_crit * sample_std/np.sqrt(len(sample_data)))\n",
    "\n",
    "# Bootstrap CI\n",
    "bootstrap_means, boot_ci_lower, boot_ci_upper = bootstrap_ci(sample_data)\n",
    "\n",
    "# Plot bootstrap distribution and CIs\n",
    "ax2.hist(bootstrap_means, bins=50, alpha=0.7, density=True, color='lightblue', \n",
    "         edgecolor='black', label='Bootstrap distribution')\n",
    "ax2.axvline(sample_mean, color='red', linestyle='-', linewidth=2, label=f'Sample mean: {sample_mean:.2f}')\n",
    "ax2.axvline(100, color='green', linestyle='--', linewidth=2, label='True mean: 100')\n",
    "ax2.axvline(t_ci[0], color='orange', linestyle=':', linewidth=2, label=f'T-CI: [{t_ci[0]:.2f}, {t_ci[1]:.2f}]')\n",
    "ax2.axvline(t_ci[1], color='orange', linestyle=':', linewidth=2)\n",
    "ax2.axvline(boot_ci_lower, color='purple', linestyle='-.', linewidth=2, label=f'Bootstrap CI: [{boot_ci_lower:.2f}, {boot_ci_upper:.2f}]')\n",
    "ax2.axvline(boot_ci_upper, color='purple', linestyle='-.', linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Sample Mean')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Bootstrap Distribution and Confidence Intervals', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfidence Interval Comparison:\")\n",
    "print(f\"T-distribution CI: [{t_ci[0]:.3f}, {t_ci[1]:.3f}]\")\n",
    "print(f\"Bootstrap CI: [{boot_ci_lower:.3f}, {boot_ci_upper:.3f}]\")\n",
    "print(f\"CI width difference: {(t_ci[1] - t_ci[0]) - (boot_ci_upper - boot_ci_lower):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Cross-Validation\n",
    "\n",
    "Statistical methods for assessing model performance and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression dataset\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=10, random_state=42)\n",
    "X = X.flatten()\n",
    "\n",
    "# Different model complexities\n",
    "models = {\n",
    "    'Linear': Pipeline([('poly', PolynomialFeatures(1)), ('reg', LinearRegression())]),\n",
    "    'Polynomial (degree 3)': Pipeline([('poly', PolynomialFeatures(3)), ('reg', LinearRegression())]),\n",
    "    'Polynomial (degree 10)': Pipeline([('poly', PolynomialFeatures(10)), ('reg', LinearRegression())]),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Cross-validation evaluation\n",
    "def evaluate_models(models, X, y, cv_folds=5):\n",
    "    \"\"\"Evaluate models using cross-validation\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Reshape X for sklearn if needed\n",
    "        X_reshaped = X.reshape(-1, 1) if X.ndim == 1 else X\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        cv_scores = cross_val_score(model, X_reshaped, y, cv=cv_folds, \n",
    "                                  scoring='neg_mean_squared_error')\n",
    "        cv_scores = -cv_scores  # Convert to positive MSE\n",
    "        \n",
    "        results[name] = {\n",
    "            'cv_scores': cv_scores,\n",
    "            'mean_cv_score': np.mean(cv_scores),\n",
    "            'std_cv_score': np.std(cv_scores),\n",
    "            'model': model\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate models\n",
    "results = evaluate_models(models, X, y)\n",
    "\n",
    "# Learning curves\n",
    "def plot_learning_curves(models, X, y):\n",
    "    \"\"\"Plot learning curves for different models\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    X_reshaped = X.reshape(-1, 1) if X.ndim == 1 else X\n",
    "    \n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        train_sizes, train_scores, val_scores = learning_curve(\n",
    "            model, X_reshaped, y, cv=5, n_jobs=-1,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "            scoring='neg_mean_squared_error'\n",
    "        )\n",
    "        \n",
    "        # Convert to positive MSE\n",
    "        train_scores = -train_scores\n",
    "        val_scores = -val_scores\n",
    "        \n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        axes[idx].plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "        axes[idx].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, \n",
    "                              alpha=0.1, color='blue')\n",
    "        \n",
    "        axes[idx].plot(train_sizes, val_mean, 'o-', color='red', label='Validation score')\n",
    "        axes[idx].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, \n",
    "                              alpha=0.1, color='red')\n",
    "        \n",
    "        axes[idx].set_xlabel('Training Set Size')\n",
    "        axes[idx].set_ylabel('Mean Squared Error')\n",
    "        axes[idx].set_title(f'Learning Curve: {name}', fontweight='bold')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(models, X, y)\n",
    "\n",
    "# Model comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Cross-validation scores comparison\n",
    "model_names = list(results.keys())\n",
    "cv_means = [results[name]['mean_cv_score'] for name in model_names]\n",
    "cv_stds = [results[name]['std_cv_score'] for name in model_names]\n",
    "\n",
    "bars = ax1.bar(model_names, cv_means, yerr=cv_stds, capsize=5, alpha=0.7, \n",
    "              color=['blue', 'green', 'red', 'orange'], edgecolor='black')\n",
    "ax1.set_ylabel('Mean Squared Error')\n",
    "ax1.set_title('Cross-Validation Performance Comparison', fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean, std in zip(bars, cv_means, cv_stds):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + std + 5,\n",
    "             f'{mean:.0f}±{std:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Box plots of CV scores\n",
    "cv_scores_list = [results[name]['cv_scores'] for name in model_names]\n",
    "bp = ax2.boxplot(cv_scores_list, labels=model_names, patch_artist=True)\n",
    "\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightsalmon']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "ax2.set_ylabel('Mean Squared Error')\n",
    "ax2.set_title('Distribution of CV Scores', fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "for name, result in results.items():\n",
    "    print(f\"{name:20}: MSE = {result['mean_cv_score']:.1f} ± {result['std_cv_score']:.1f}\")\n",
    "\n",
    "# Statistical significance test\n",
    "best_model = min(results.keys(), key=lambda x: results[x]['mean_cv_score'])\n",
    "print(f\"\\nBest model: {best_model}\")\n",
    "\n",
    "# Paired t-test between best model and others\n",
    "print(\"\\nStatistical significance tests (paired t-test):\")\n",
    "best_scores = results[best_model]['cv_scores']\n",
    "for name, result in results.items():\n",
    "    if name != best_model:\n",
    "        t_stat, p_value = stats.ttest_rel(best_scores, result['cv_scores'])\n",
    "        significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "        print(f\"{best_model} vs {name:15}: t={t_stat:6.2f}, p={p_value:.4f} {significance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bias-Variance Tradeoff\n",
    "\n",
    "Understanding the fundamental tradeoff in machine learning model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias-variance decomposition simulation\n",
    "def bias_variance_decomposition(n_datasets=100, n_test_points=50, noise_level=0.3):\n",
    "    \"\"\"Simulate bias-variance decomposition for polynomial models\"\"\"\n",
    "    \n",
    "    # True function\n",
    "    def true_function(x):\n",
    "        return 1.5 * x + 0.5 * x**2 - 0.1 * x**3\n",
    "    \n",
    "    # Test points\n",
    "    x_test = np.linspace(-2, 2, n_test_points)\n",
    "    y_true = true_function(x_test)\n",
    "    \n",
    "    # Different polynomial degrees\n",
    "    degrees = [1, 3, 5, 10]\n",
    "    results = {}\n",
    "    \n",
    "    for degree in degrees:\n",
    "        predictions = []\n",
    "        \n",
    "        for _ in range(n_datasets):\n",
    "            # Generate training data\n",
    "            x_train = np.random.uniform(-2, 2, 50)\n",
    "            y_train = true_function(x_train) + np.random.normal(0, noise_level, 50)\n",
    "            \n",
    "            # Fit polynomial model\n",
    "            coeffs = np.polyfit(x_train, y_train, degree)\n",
    "            y_pred = np.polyval(coeffs, x_test)\n",
    "            predictions.append(y_pred)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # Calculate bias, variance, and noise\n",
    "        mean_prediction = np.mean(predictions, axis=0)\n",
    "        bias_squared = (mean_prediction - y_true) ** 2\n",
    "        variance = np.var(predictions, axis=0)\n",
    "        noise = noise_level ** 2\n",
    "        \n",
    "        total_error = bias_squared + variance + noise\n",
    "        \n",
    "        results[degree] = {\n",
    "            'predictions': predictions,\n",
    "            'mean_prediction': mean_prediction,\n",
    "            'bias_squared': np.mean(bias_squared),\n",
    "            'variance': np.mean(variance),\n",
    "            'noise': noise,\n",
    "            'total_error': np.mean(total_error)\n",
    "        }\n",
    "    \n",
    "    return x_test, y_true, results\n",
    "\n",
    "# Run bias-variance analysis\n",
    "x_test, y_true, bv_results = bias_variance_decomposition()\n",
    "\n",
    "# Visualize bias-variance tradeoff\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "for idx, (degree, color) in enumerate(zip([1, 3, 5, 10], colors)):\n",
    "    result = bv_results[degree]\n",
    "    \n",
    "    # Plot some individual predictions\n",
    "    for i in range(min(20, len(result['predictions']))):\n",
    "        axes[idx].plot(x_test, result['predictions'][i], color=color, alpha=0.1)\n",
    "    \n",
    "    # Plot mean prediction and true function\n",
    "    axes[idx].plot(x_test, result['mean_prediction'], color=color, linewidth=3, \n",
    "                  label=f'Mean prediction (degree {degree})')\n",
    "    axes[idx].plot(x_test, y_true, 'k--', linewidth=2, label='True function')\n",
    "    \n",
    "    axes[idx].set_title(f'Degree {degree}: Bias²={result[\"bias_squared\"]:.3f}, '\n",
    "                       f'Var={result[\"variance\"]:.3f}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('x')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Bias-Variance Tradeoff: Individual Predictions and Averages', \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary plot of bias-variance tradeoff\n",
    "degrees = list(bv_results.keys())\n",
    "bias_squared = [bv_results[d]['bias_squared'] for d in degrees]\n",
    "variance = [bv_results[d]['variance'] for d in degrees]\n",
    "noise = [bv_results[d]['noise'] for d in degrees]\n",
    "total_error = [bv_results[d]['total_error'] for d in degrees]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bias-variance components\n",
    "ax1.plot(degrees, bias_squared, 'o-', color='red', linewidth=2, markersize=8, label='Bias²')\n",
    "ax1.plot(degrees, variance, 's-', color='blue', linewidth=2, markersize=8, label='Variance')\n",
    "ax1.plot(degrees, noise, '^-', color='green', linewidth=2, markersize=8, label='Noise')\n",
    "ax1.plot(degrees, total_error, 'd-', color='black', linewidth=3, markersize=8, label='Total Error')\n",
    "\n",
    "ax1.set_xlabel('Polynomial Degree (Model Complexity)')\n",
    "ax1.set_ylabel('Error')\n",
    "ax1.set_title('Bias-Variance Decomposition', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Stacked bar chart\n",
    "width = 0.6\n",
    "ax2.bar(degrees, bias_squared, width, label='Bias²', color='red', alpha=0.7)\n",
    "ax2.bar(degrees, variance, width, bottom=bias_squared, label='Variance', color='blue', alpha=0.7)\n",
    "ax2.bar(degrees, noise, width, bottom=np.array(bias_squared) + np.array(variance), \n",
    "       label='Noise', color='green', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('Polynomial Degree')\n",
    "ax2.set_ylabel('Error Components')\n",
    "ax2.set_title('Error Decomposition (Stacked)', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print numerical results\n",
    "print(\"Bias-Variance Decomposition Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Degree':>8} {'Bias²':>10} {'Variance':>10} {'Noise':>8} {'Total':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for degree in degrees:\n",
    "    result = bv_results[degree]\n",
    "    print(f\"{degree:>8} {result['bias_squared']:>10.4f} {result['variance']:>10.4f} \"\n",
    "          f\"{result['noise']:>8.4f} {result['total_error']:>10.4f}\")\n",
    "\n",
    "optimal_degree = min(degrees, key=lambda d: bv_results[d]['total_error'])\n",
    "print(f\"\\nOptimal polynomial degree: {optimal_degree}\")\n",
    "print(f\"Minimum total error: {bv_results[optimal_degree]['total_error']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. A/B Testing and Experimental Design\n",
    "\n",
    "Statistical methods for comparing treatments and making decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Testing simulation\n",
    "def ab_test_simulation(control_rate=0.10, treatment_rate=0.12, n_control=1000, n_treatment=1000):\n",
    "    \"\"\"Simulate A/B test with conversion rates\"\"\"\n",
    "    \n",
    "    # Generate data\n",
    "    control_conversions = np.random.binomial(n_control, control_rate)\n",
    "    treatment_conversions = np.random.binomial(n_treatment, treatment_rate)\n",
    "    \n",
    "    # Calculate rates\n",
    "    control_conv_rate = control_conversions / n_control\n",
    "    treatment_conv_rate = treatment_conversions / n_treatment\n",
    "    \n",
    "    # Statistical test (two-proportion z-test)\n",
    "    pooled_rate = (control_conversions + treatment_conversions) / (n_control + n_treatment)\n",
    "    pooled_se = np.sqrt(pooled_rate * (1 - pooled_rate) * (1/n_control + 1/n_treatment))\n",
    "    \n",
    "    z_stat = (treatment_conv_rate - control_conv_rate) / pooled_se\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "    \n",
    "    # Confidence interval for difference\n",
    "    diff = treatment_conv_rate - control_conv_rate\n",
    "    se_diff = np.sqrt(control_conv_rate * (1 - control_conv_rate) / n_control +\n",
    "                     treatment_conv_rate * (1 - treatment_conv_rate) / n_treatment)\n",
    "    ci_lower = diff - 1.96 * se_diff\n",
    "    ci_upper = diff + 1.96 * se_diff\n",
    "    \n",
    "    return {\n",
    "        'control_conversions': control_conversions,\n",
    "        'treatment_conversions': treatment_conversions,\n",
    "        'control_rate': control_conv_rate,\n",
    "        'treatment_rate': treatment_conv_rate,\n",
    "        'difference': diff,\n",
    "        'z_stat': z_stat,\n",
    "        'p_value': p_value,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "\n",
    "# Power analysis for A/B testing\n",
    "def ab_test_power_analysis(control_rate=0.10, effect_sizes=None, alpha=0.05, power=0.8):\n",
    "    \"\"\"Calculate required sample sizes for different effect sizes\"\"\"\n",
    "    if effect_sizes is None:\n",
    "        effect_sizes = np.array([0.001, 0.005, 0.01, 0.02, 0.03, 0.05])\n",
    "    \n",
    "    sample_sizes = []\n",
    "    \n",
    "    for effect in effect_sizes:\n",
    "        treatment_rate = control_rate + effect\n",
    "        \n",
    "        # Calculate required sample size using formula\n",
    "        p_avg = (control_rate + treatment_rate) / 2\n",
    "        z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "        z_beta = stats.norm.ppf(power)\n",
    "        \n",
    "        n = (z_alpha * np.sqrt(2 * p_avg * (1 - p_avg)) + \n",
    "             z_beta * np.sqrt(control_rate * (1 - control_rate) + treatment_rate * (1 - treatment_rate)))**2 / effect**2\n",
    "        \n",
    "        sample_sizes.append(int(np.ceil(n)))\n",
    "    \n",
    "    return effect_sizes, sample_sizes\n",
    "\n",
    "# Run A/B test examples\n",
    "test_scenarios = [\n",
    "    (0.10, 0.12, \"Significant effect\"),\n",
    "    (0.10, 0.105, \"Small effect\"),\n",
    "    (0.10, 0.10, \"No effect\")\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Run multiple A/B tests\n",
    "for idx, (control_rate, treatment_rate, description) in enumerate(test_scenarios[:3]):\n",
    "    if idx < 3:\n",
    "        result = ab_test_simulation(control_rate, treatment_rate)\n",
    "        \n",
    "        ax = axes[idx//2, idx%2]\n",
    "        \n",
    "        # Bar plot of conversion rates\n",
    "        rates = [result['control_rate'], result['treatment_rate']]\n",
    "        labels = ['Control', 'Treatment']\n",
    "        colors = ['lightblue', 'lightgreen' if result['significant'] else 'lightcoral']\n",
    "        \n",
    "        bars = ax.bar(labels, rates, color=colors, edgecolor='black', alpha=0.7)\n",
    "        \n",
    "        # Add error bars (confidence intervals)\n",
    "        for i, (rate, n) in enumerate([(result['control_rate'], 1000), (result['treatment_rate'], 1000)]):\n",
    "            se = np.sqrt(rate * (1 - rate) / n)\n",
    "            ci = 1.96 * se\n",
    "            ax.errorbar(i, rate, yerr=ci, fmt='none', color='black', capsize=5)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, rate in zip(bars, rates):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
    "                   f'{rate:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax.set_ylabel('Conversion Rate')\n",
    "        ax.set_title(f'{description}\\np-value: {result[\"p_value\"]:.4f}, '\n",
    "                    f'Difference: {result[\"difference\"]:.3f}', fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        print(f\"{description}:\")\n",
    "        print(f\"  Control rate: {result['control_rate']:.4f}\")\n",
    "        print(f\"  Treatment rate: {result['treatment_rate']:.4f}\")\n",
    "        print(f\"  Difference: {result['difference']:.4f} [{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]\")\n",
    "        print(f\"  Z-statistic: {result['z_stat']:.3f}\")\n",
    "        print(f\"  P-value: {result['p_value']:.4f}\")\n",
    "        print(f\"  Significant: {result['significant']}\")\n",
    "        print()\n",
    "\n",
    "# Power analysis plot\n",
    "ax = axes[1, 1]\n",
    "effect_sizes, sample_sizes = ab_test_power_analysis()\n",
    "\n",
    "ax.plot(effect_sizes * 100, sample_sizes, 'bo-', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Effect Size (percentage points)')\n",
    "ax.set_ylabel('Required Sample Size per Group')\n",
    "ax.set_title('Sample Size Requirements for A/B Tests\\n(80% Power, 5% Significance)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Add annotations\n",
    "for effect, n in zip(effect_sizes, sample_sizes):\n",
    "    ax.annotate(f'{n:,}', (effect*100, n), textcoords=\"offset points\", \n",
    "               xytext=(0,10), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sequential testing simulation\n",
    "def sequential_ab_test(control_rate=0.10, treatment_rate=0.12, max_n=2000, alpha=0.05):\n",
    "    \"\"\"Simulate sequential A/B testing\"\"\"\n",
    "    n_points = []\n",
    "    p_values = []\n",
    "    \n",
    "    control_successes = 0\n",
    "    treatment_successes = 0\n",
    "    \n",
    "    for n in range(50, max_n, 50):  # Check every 50 samples\n",
    "        # Add new samples\n",
    "        new_control = np.random.binomial(50, control_rate)\n",
    "        new_treatment = np.random.binomial(50, treatment_rate)\n",
    "        \n",
    "        control_successes += new_control\n",
    "        treatment_successes += new_treatment\n",
    "        \n",
    "        # Calculate current rates and test\n",
    "        control_conv_rate = control_successes / n\n",
    "        treatment_conv_rate = treatment_successes / n\n",
    "        \n",
    "        if n > 100:  # Minimum sample size\n",
    "            pooled_rate = (control_successes + treatment_successes) / (2 * n)\n",
    "            pooled_se = np.sqrt(pooled_rate * (1 - pooled_rate) * (2/n))\n",
    "            \n",
    "            if pooled_se > 0:\n",
    "                z_stat = (treatment_conv_rate - control_conv_rate) / pooled_se\n",
    "                p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "            else:\n",
    "                p_value = 1.0\n",
    "            \n",
    "            n_points.append(n)\n",
    "            p_values.append(p_value)\n",
    "    \n",
    "    return n_points, p_values\n",
    "\n",
    "# Run sequential test\n",
    "n_seq, p_seq = sequential_ab_test()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.plot(n_seq, p_seq, 'b-', linewidth=2, label='P-value over time')\n",
    "ax.axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='Significance threshold (α=0.05)')\n",
    "ax.fill_between(n_seq, 0, 0.05, alpha=0.2, color='red', label='Rejection region')\n",
    "\n",
    "ax.set_xlabel('Sample Size per Group')\n",
    "ax.set_ylabel('P-value')\n",
    "ax.set_title('Sequential A/B Testing: P-value Evolution', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find when test becomes significant\n",
    "significant_at = None\n",
    "for n, p in zip(n_seq, p_seq):\n",
    "    if p < 0.05:\n",
    "        significant_at = n\n",
    "        break\n",
    "\n",
    "if significant_at:\n",
    "    print(f\"Test became significant at n = {significant_at} per group\")\n",
    "else:\n",
    "    print(\"Test did not reach significance within the sample size limit\")\n",
    "\n",
    "print(f\"Final p-value: {p_seq[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Descriptive Statistics**: Foundation for understanding data distributions\n",
    "2. **Hypothesis Testing**: Statistical framework for making decisions under uncertainty\n",
    "3. **Confidence Intervals**: Quantify uncertainty in parameter estimates\n",
    "4. **Cross-Validation**: Essential for honest model evaluation\n",
    "5. **Bias-Variance Tradeoff**: Fundamental principle governing model complexity\n",
    "6. **A/B Testing**: Rigorous experimental design for comparing treatments\n",
    "7. **Statistical Power**: Important for planning experiments and interpreting results\n",
    "\n",
    "## Applications in Machine Learning\n",
    "\n",
    "- **Model Selection**: Use statistical tests to compare model performance\n",
    "- **Feature Selection**: Statistical significance of features\n",
    "- **Hyperparameter Tuning**: Cross-validation for parameter optimization\n",
    "- **Confidence Estimation**: Bootstrap methods for uncertainty quantification\n",
    "- **Experimental Design**: A/B testing for ML system improvements\n",
    "- **Outlier Detection**: Statistical methods for identifying anomalies\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "- **Multiple Comparisons**: Adjust p-values when testing many hypotheses\n",
    "- **Data Snooping**: Avoid overfitting to validation data\n",
    "- **Cherry Picking**: Report all analyses, not just significant results\n",
    "- **Correlation vs Causation**: Statistical association ≠ causal relationship\n",
    "- **Sample Size**: Ensure adequate power for meaningful conclusions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Study non-parametric statistical methods\n",
    "- Learn about multiple testing correction\n",
    "- Explore Bayesian statistics and MCMC\n",
    "- Practice with real-world datasets\n",
    "- Understand causal inference methods\n",
    "\n",
    "---\n",
    "\n",
    "**Statistical Concepts Covered:**\n",
    "- Descriptive statistics and data exploration\n",
    "- Hypothesis testing and p-values\n",
    "- Confidence intervals and bootstrap methods\n",
    "- Cross-validation and model evaluation\n",
    "- Bias-variance decomposition\n",
    "- Experimental design and A/B testing\n",
    "- Statistical power and sample size calculation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}