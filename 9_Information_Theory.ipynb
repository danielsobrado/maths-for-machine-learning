{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1d65b93",
   "metadata": {},
   "source": [
    "# Information Theory for Machine Learning\n",
    "## Entropy-Based Metrics and the Information Bottleneck Principle\n",
    "\n",
    "Welcome to the **mathematical science of information**! Information theory provides the fundamental framework for understanding how much information is contained in data and how efficiently we can process it.\n",
    "\n",
    "### What You'll Master\n",
    "By the end of this notebook, you'll understand:\n",
    "1. **Entropy and information** - Quantifying uncertainty and surprise\n",
    "2. **Mutual information** - Measuring statistical dependence\n",
    "3. **KL divergence** - Comparing probability distributions\n",
    "4. **Information bottleneck** - The principle behind representation learning\n",
    "5. **Channel capacity** - Limits of information transmission\n",
    "6. **Applications in ML** - From feature selection to deep learning\n",
    "\n",
    "### Why This is Revolutionary\n",
    "- **Deep learning** can be understood through information bottleneck theory\n",
    "- **Feature selection** uses mutual information to find relevant variables\n",
    "- **Model compression** applies information theory to reduce model size\n",
    "- **Generalization** is fundamentally about information processing\n",
    "\n",
    "### Real-World Applications\n",
    "- **Data compression**: JPEG, MP3, ZIP files\n",
    "- **Cryptography**: Measuring randomness and security\n",
    "- **Neural networks**: Understanding what layers learn\n",
    "- **Reinforcement learning**: Information-theoretic exploration\n",
    "\n",
    "Let's decode the mathematics of information! 📡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy, multivariate_normal\n",
    "from scipy.special import rel_entr\n",
    "from sklearn.datasets import make_classification, load_digits\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Mathematical constants\n",
    "LOG2 = np.log(2)\n",
    "NATS_TO_BITS = 1 / LOG2\n",
    "\n",
    "print(\"📡 Information Theory toolkit loaded!\")\n",
    "print(\"Ready to measure information and uncertainty!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872d960",
   "metadata": {},
   "source": [
    "## 1. Entropy: The Fundamental Measure of Information\n",
    "\n",
    "### What is Entropy?\n",
    "**Entropy** measures the average amount of information (or uncertainty) in a random variable.\n",
    "\n",
    "**Shannon Entropy Formula**:\n",
    "```\n",
    "H(X) = -∑ P(x) log P(x)\n",
    "```\n",
    "\n",
    "### Intuitive Understanding\n",
    "- **High entropy**: Outcome is unpredictable (fair coin flip)\n",
    "- **Low entropy**: Outcome is predictable (biased coin)\n",
    "- **Zero entropy**: Outcome is certain (deterministic)\n",
    "\n",
    "### Units of Measurement\n",
    "- **Bits**: log₂ (most common in CS)\n",
    "- **Nats**: ln (natural logarithm)\n",
    "- **Dits**: log₁₀ (decimal digits)\n",
    "\n",
    "### Properties of Entropy\n",
    "1. **Non-negative**: H(X) ≥ 0\n",
    "2. **Maximum**: H(X) ≤ log|X| (uniform distribution)\n",
    "3. **Concave**: More spread out → higher entropy\n",
    "4. **Additive**: H(X,Y) = H(X) + H(Y) if X ⊥ Y\n",
    "\n",
    "### Real-World Analogy\n",
    "Think of entropy as **\"surprise level\"**:\n",
    "- Seeing the sun rise: Low surprise (low entropy)\n",
    "- Winning the lottery: High surprise (high entropy)\n",
    "- Weather in desert: Low entropy (predictable)\n",
    "- Weather in England: High entropy (unpredictable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93182fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_entropy_concepts():\n",
    "    \"\"\"Explore entropy with interactive examples\"\"\"\n",
    "    \n",
    "    print(\"📊 Entropy: Measuring Information and Uncertainty\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Helper function to calculate entropy\n",
    "    def calculate_entropy(probabilities, base=2):\n",
    "        \"\"\"Calculate entropy with specified base\"\"\"\n",
    "        probabilities = np.array(probabilities)\n",
    "        # Remove zero probabilities to avoid log(0)\n",
    "        probabilities = probabilities[probabilities > 0]\n",
    "        if base == 2:\n",
    "            return -np.sum(probabilities * np.log2(probabilities))\n",
    "        elif base == np.e:\n",
    "            return -np.sum(probabilities * np.log(probabilities))\n",
    "        else:\n",
    "            return -np.sum(probabilities * np.log(probabilities) / np.log(base))\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Coin flip entropy\n",
    "    print(\"\\n1. Coin Flip Entropy\")\n",
    "    print(\"   Fair coin: Maximum entropy\")\n",
    "    print(\"   Biased coin: Lower entropy\")\n",
    "    \n",
    "    p_values = np.linspace(0.01, 0.99, 100)\n",
    "    entropies = [calculate_entropy([p, 1-p]) for p in p_values]\n",
    "    \n",
    "    axes[0, 0].plot(p_values, entropies, 'b-', linewidth=2)\n",
    "    axes[0, 0].axvline(x=0.5, color='r', linestyle='--', alpha=0.7, label='Fair coin (max entropy)')\n",
    "    axes[0, 0].scatter([0.5], [1.0], color='red', s=100, zorder=5)\n",
    "    axes[0, 0].set_xlabel('P(Heads)')\n",
    "    axes[0, 0].set_ylabel('Entropy (bits)')\n",
    "    axes[0, 0].set_title('Coin Flip Entropy')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Show specific examples\n",
    "    examples = [(0.1, 'Biased'), (0.5, 'Fair'), (0.9, 'Biased')]\n",
    "    for p, label in examples:\n",
    "        h = calculate_entropy([p, 1-p])\n",
    "        print(f\"   P(H)={p}: H = {h:.3f} bits\")\n",
    "    \n",
    "    # 2. Dice entropy\n",
    "    print(\"\\n2. Dice Roll Entropy\")\n",
    "    print(\"   Fair die vs loaded die\")\n",
    "    \n",
    "    # Fair die\n",
    "    fair_die = [1/6] * 6\n",
    "    fair_entropy = calculate_entropy(fair_die)\n",
    "    \n",
    "    # Loaded die\n",
    "    loaded_die = [0.5, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "    loaded_entropy = calculate_entropy(loaded_die)\n",
    "    \n",
    "    x_pos = np.arange(6)\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 1].bar(x_pos - width/2, fair_die, width, label=f'Fair die (H={fair_entropy:.2f})', alpha=0.8)\n",
    "    axes[0, 1].bar(x_pos + width/2, loaded_die, width, label=f'Loaded die (H={loaded_entropy:.2f})', alpha=0.8)\n",
    "    axes[0, 1].set_xlabel('Dice Face')\n",
    "    axes[0, 1].set_ylabel('Probability')\n",
    "    axes[0, 1].set_title('Fair vs Loaded Dice')\n",
    "    axes[0, 1].set_xticks(x_pos)\n",
    "    axes[0, 1].set_xticklabels([f'{i+1}' for i in range(6)])\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"   Fair die: H = {fair_entropy:.3f} bits\")\n",
    "    print(f\"   Loaded die: H = {loaded_entropy:.3f} bits\")\n",
    "    \n",
    "    # 3. Text entropy\n",
    "    print(\"\\n3. Text Entropy (English vs Random)\")\n",
    "    \n",
    "    # English letter frequencies (approximate)\n",
    "    english_freq = [0.08167, 0.01492, 0.02782, 0.04253, 0.12, 0.02228, 0.02015, 0.06094, 0.06966, 0.00153,\n",
    "                   0.00772, 0.04025, 0.02406, 0.06749, 0.07507, 0.01929, 0.00095, 0.05987, 0.06327, 0.09056,\n",
    "                   0.02758, 0.00978, 0.02360, 0.00150, 0.01974, 0.00074]\n",
    "    \n",
    "    # Random text (uniform distribution)\n",
    "    random_freq = [1/26] * 26\n",
    "    \n",
    "    english_entropy = calculate_entropy(english_freq)\n",
    "    random_entropy = calculate_entropy(random_freq)\n",
    "    \n",
    "    letters = [chr(ord('A') + i) for i in range(26)]\n",
    "    x_letters = np.arange(26)\n",
    "    \n",
    "    axes[0, 2].bar(x_letters, english_freq, alpha=0.7, label=f'English (H={english_entropy:.2f})')\n",
    "    axes[0, 2].axhline(y=1/26, color='red', linestyle='--', alpha=0.7, label=f'Random (H={random_entropy:.2f})')\n",
    "    axes[0, 2].set_xlabel('Letter')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_title('Letter Frequency: English vs Random')\n",
    "    axes[0, 2].set_xticks(x_letters[::3])\n",
    "    axes[0, 2].set_xticklabels(letters[::3])\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"   English text: H = {english_entropy:.3f} bits per letter\")\n",
    "    print(f\"   Random text: H = {random_entropy:.3f} bits per letter\")\n",
    "    \n",
    "    # 4. Conditional entropy\n",
    "    print(\"\\n4. Conditional Entropy: H(Y|X)\")\n",
    "    print(\"   Entropy of Y given knowledge of X\")\n",
    "    \n",
    "    # Create a simple example: weather prediction\n",
    "    # X = season, Y = temperature\n",
    "    seasons = ['Spring', 'Summer', 'Fall', 'Winter']\n",
    "    temp_ranges = ['Cold', 'Mild', 'Warm', 'Hot']\n",
    "    \n",
    "    # Joint probability P(season, temperature)\n",
    "    joint_prob = np.array([\n",
    "        [0.05, 0.15, 0.05, 0.00],  # Spring\n",
    "        [0.00, 0.05, 0.15, 0.05],  # Summer\n",
    "        [0.05, 0.15, 0.05, 0.00],  # Fall\n",
    "        [0.15, 0.10, 0.00, 0.00]   # Winter\n",
    "    ])\n",
    "    \n",
    "    # Marginal probabilities\n",
    "    p_season = joint_prob.sum(axis=1)\n",
    "    p_temp = joint_prob.sum(axis=0)\n",
    "    \n",
    "    # Calculate entropies\n",
    "    H_temp = calculate_entropy(p_temp)\n",
    "    \n",
    "    # Conditional entropy H(Temp|Season)\n",
    "    H_temp_given_season = 0\n",
    "    for i, season_prob in enumerate(p_season):\n",
    "        if season_prob > 0:\n",
    "            conditional_probs = joint_prob[i] / season_prob\n",
    "            H_temp_given_season += season_prob * calculate_entropy(conditional_probs)\n",
    "    \n",
    "    # Visualize joint distribution\n",
    "    im = axes[1, 0].imshow(joint_prob, cmap='Blues', aspect='auto')\n",
    "    axes[1, 0].set_xticks(range(4))\n",
    "    axes[1, 0].set_yticks(range(4))\n",
    "    axes[1, 0].set_xticklabels(temp_ranges)\n",
    "    axes[1, 0].set_yticklabels(seasons)\n",
    "    axes[1, 0].set_xlabel('Temperature')\n",
    "    axes[1, 0].set_ylabel('Season')\n",
    "    axes[1, 0].set_title('Joint Distribution P(Season, Temp)')\n",
    "    \n",
    "    # Add probability values to heatmap\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axes[1, 0].text(j, i, f'{joint_prob[i, j]:.2f}', \n",
    "                           ha='center', va='center', color='white' if joint_prob[i, j] > 0.1 else 'black')\n",
    "    \n",
    "    print(f\"   H(Temperature) = {H_temp:.3f} bits\")\n",
    "    print(f\"   H(Temperature|Season) = {H_temp_given_season:.3f} bits\")\n",
    "    print(f\"   Information gain = {H_temp - H_temp_given_season:.3f} bits\")\n",
    "    \n",
    "    # 5. Cross-entropy and KL divergence\n",
    "    print(\"\\n5. Cross-Entropy and KL Divergence\")\n",
    "    print(\"   Measuring difference between distributions\")\n",
    "    \n",
    "    # True distribution (uniform)\n",
    "    true_dist = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    \n",
    "    # Different predicted distributions\n",
    "    pred_dists = {\n",
    "        'Perfect': np.array([0.25, 0.25, 0.25, 0.25]),\n",
    "        'Close': np.array([0.3, 0.3, 0.2, 0.2]),\n",
    "        'Poor': np.array([0.7, 0.1, 0.1, 0.1]),\n",
    "        'Terrible': np.array([0.95, 0.02, 0.02, 0.01])\n",
    "    }\n",
    "    \n",
    "    x_cat = np.arange(4)\n",
    "    categories = ['A', 'B', 'C', 'D']\n",
    "    \n",
    "    axes[1, 1].bar(x_cat - 0.3, true_dist, 0.2, label='True', alpha=0.8)\n",
    "    \n",
    "    colors = ['green', 'yellow', 'orange', 'red']\n",
    "    for i, (name, pred_dist) in enumerate(pred_dists.items()):\n",
    "        offset = -0.1 + i * 0.2\n",
    "        axes[1, 1].bar(x_cat + offset, pred_dist, 0.15, label=name, alpha=0.7, color=colors[i])\n",
    "        \n",
    "        # Calculate KL divergence\n",
    "        kl_div = np.sum(true_dist * np.log(true_dist / pred_dist))\n",
    "        cross_entropy = -np.sum(true_dist * np.log(pred_dist))\n",
    "        print(f\"   {name}: KL(true||pred) = {kl_div:.3f}, Cross-entropy = {cross_entropy:.3f}\")\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Category')\n",
    "    axes[1, 1].set_ylabel('Probability')\n",
    "    axes[1, 1].set_title('Cross-Entropy: True vs Predicted Distributions')\n",
    "    axes[1, 1].set_xticks(x_cat)\n",
    "    axes[1, 1].set_xticklabels(categories)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Information-theoretic quantities relationships\n",
    "    print(\"\\n6. Information-Theoretic Relationships\")\n",
    "    \n",
    "    # Create Venn diagram showing relationships\n",
    "    fig_venn = plt.figure(figsize=(8, 6))\n",
    "    ax_venn = fig_venn.add_subplot(111)\n",
    "    \n",
    "    # Draw circles for X and Y\n",
    "    circle1 = plt.Circle((0.4, 0.5), 0.3, fill=False, linewidth=2, color='blue')\n",
    "    circle2 = plt.Circle((0.6, 0.5), 0.3, fill=False, linewidth=2, color='red')\n",
    "    ax_venn.add_patch(circle1)\n",
    "    ax_venn.add_patch(circle2)\n",
    "    \n",
    "    # Add labels\n",
    "    ax_venn.text(0.25, 0.5, 'H(X|Y)', fontsize=12, ha='center', va='center')\n",
    "    ax_venn.text(0.75, 0.5, 'H(Y|X)', fontsize=12, ha='center', va='center')\n",
    "    ax_venn.text(0.5, 0.5, 'I(X;Y)', fontsize=12, ha='center', va='center', weight='bold')\n",
    "    ax_venn.text(0.15, 0.8, 'H(X)', fontsize=14, ha='center', color='blue', weight='bold')\n",
    "    ax_venn.text(0.85, 0.8, 'H(Y)', fontsize=14, ha='center', color='red', weight='bold')\n",
    "    ax_venn.text(0.5, 0.15, 'H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)', fontsize=12, ha='center')\n",
    "    ax_venn.text(0.5, 0.05, 'I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)', fontsize=12, ha='center', weight='bold')\n",
    "    \n",
    "    ax_venn.set_xlim(0, 1)\n",
    "    ax_venn.set_ylim(0, 1)\n",
    "    ax_venn.set_aspect('equal')\n",
    "    ax_venn.axis('off')\n",
    "    ax_venn.set_title('Information-Theoretic Quantities', fontsize=16, weight='bold')\n",
    "    \n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show the Venn diagram\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n🎯 Key Information Theory Concepts:\")\n",
    "    print(\"• Entropy measures uncertainty/information content\")\n",
    "    print(\"• Conditional entropy: uncertainty remaining after observing another variable\")\n",
    "    print(\"• Mutual information: reduction in uncertainty due to another variable\")\n",
    "    print(\"• KL divergence: 'distance' between probability distributions\")\n",
    "    print(\"• Cross-entropy: expected message length using wrong distribution\")\n",
    "\n",
    "demonstrate_entropy_concepts()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
