{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa09625",
   "metadata": {},
   "source": [
    "# Applied Measure Theory for Machine Learning\n",
    "## Rigorous Probability Foundations and Advanced Integration\n",
    "\n",
    "Welcome to the **mathematical foundation of modern probability**! Measure theory provides the rigorous framework that makes probability theory logically consistent and enables advanced techniques in machine learning and statistics.\n",
    "\n",
    "### What You'll Master\n",
    "By the end of this notebook, you'll understand:\n",
    "1. **Measure spaces** - The foundation of rigorous probability\n",
    "2. **Lebesgue integration** - Beyond Riemann integration\n",
    "3. **Probability measures** - Formal definition of probability\n",
    "4. **Random variables** - Measurable functions on probability spaces\n",
    "5. **Convergence theorems** - When limits and integrals commute\n",
    "6. **Radon-Nikodym theorem** - Density functions rigorously defined\n",
    "\n",
    "### Why This Matters\n",
    "- **Rigorous foundations** - Make probability theory mathematically sound\n",
    "- **Advanced ML theory** - Understand PAC learning, uniform convergence\n",
    "- **Functional analysis** - Foundation for kernel methods, reproducing Hilbert spaces\n",
    "- **Stochastic calculus** - Theoretical basis for continuous-time processes\n",
    "\n",
    "### Real-World Applications\n",
    "- **Option pricing**: It√¥ calculus and stochastic differential equations\n",
    "- **Signal processing**: L¬≤ spaces and Fourier analysis\n",
    "- **Deep learning theory**: Universal approximation theorems\n",
    "- **Quantum computing**: Probability amplitudes in Hilbert spaces\n",
    "\n",
    "Let's build the mathematical skyscraper of modern probability! üèóÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8825ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats, integrate\n",
    "from scipy.stats import norm, uniform, expon, gamma\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"plasma\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üèóÔ∏è Measure Theory toolkit loaded!\")\n",
    "print(\"Ready to build rigorous probability foundations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebac97",
   "metadata": {},
   "source": [
    "## 1. Measure Spaces: The Foundation\n",
    "\n",
    "### What is a Measure?\n",
    "A **measure** is a function that assigns a non-negative number (possibly infinite) to subsets of a space, generalizing concepts like length, area, and volume.\n",
    "\n",
    "### The Triple (Œ©, ‚Ñ±, Œº)\n",
    "A **measure space** consists of:\n",
    "1. **Œ©**: The sample space (set of all possible outcomes)\n",
    "2. **‚Ñ±**: A œÉ-algebra (collection of measurable sets)\n",
    "3. **Œº**: A measure function Œº: ‚Ñ± ‚Üí [0, ‚àû]\n",
    "\n",
    "### œÉ-Algebra Properties\n",
    "A collection ‚Ñ± of subsets of Œ© is a œÉ-algebra if:\n",
    "1. **Œ© ‚àà ‚Ñ±** (contains the whole space)\n",
    "2. **A ‚àà ‚Ñ± ‚üπ A·∂ú ‚àà ‚Ñ±** (closed under complements)\n",
    "3. **A‚ÇÅ, A‚ÇÇ, ... ‚àà ‚Ñ± ‚üπ ‚ãÉ·µ¢ A·µ¢ ‚àà ‚Ñ±** (closed under countable unions)\n",
    "\n",
    "### Measure Properties\n",
    "A function Œº: ‚Ñ± ‚Üí [0, ‚àû] is a measure if:\n",
    "1. **Œº(‚àÖ) = 0** (empty set has measure zero)\n",
    "2. **Countable additivity**: For disjoint sets A‚ÇÅ, A‚ÇÇ, ...\n",
    "   Œº(‚ãÉ·µ¢ A·µ¢) = Œ£·µ¢ Œº(A·µ¢)\n",
    "\n",
    "### Important Examples\n",
    "- **Counting measure**: Œº(A) = |A| (number of elements)\n",
    "- **Lebesgue measure**: Œº([a,b]) = b - a (length)\n",
    "- **Dirac measure**: Œº(A) = 1 if x‚ÇÄ ‚àà A, 0 otherwise\n",
    "- **Probability measure**: Œº(Œ©) = 1\n",
    "\n",
    "### Why œÉ-Algebras?\n",
    "œÉ-algebras solve the **Banach-Tarski paradox** - without them, we could \"decompose\" a ball into two balls of the same size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d914ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_measure_theory_foundations():\n",
    "    \"\"\"Explore the foundations of measure theory\"\"\"\n",
    "    \n",
    "    print(\"üìê Measure Theory: Building Rigorous Foundations\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Riemann vs Lebesgue integration\n",
    "    print(\"\\n1. Riemann vs Lebesgue Integration\")\n",
    "    print(\"   Different approaches to measuring 'area under curve'\")\n",
    "    \n",
    "    # Create a function with discontinuities\n",
    "    def discontinuous_function(x):\n",
    "        \"\"\"A function that's problematic for Riemann integration\"\"\"\n",
    "        # Thomae function (modified)\n",
    "        result = np.zeros_like(x)\n",
    "        for i, val in enumerate(x):\n",
    "            if abs(val - 0.5) < 0.01:  # Spike at 0.5\n",
    "                result[i] = 1.0\n",
    "            elif abs(val - 0.3) < 0.005:  # Smaller spike at 0.3\n",
    "                result[i] = 0.5\n",
    "            elif abs(val - 0.7) < 0.005:  # Smaller spike at 0.7\n",
    "                result[i] = 0.5\n",
    "            else:\n",
    "                result[i] = 0.1 * np.sin(10 * np.pi * val)\n",
    "        return result\n",
    "    \n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    y = discontinuous_function(x)\n",
    "    \n",
    "    # Riemann approach: vertical rectangles\n",
    "    n_riemann = 20\n",
    "    x_riemann = np.linspace(0, 1, n_riemann + 1)\n",
    "    dx = x_riemann[1] - x_riemann[0]\n",
    "    \n",
    "    # Sample function at left endpoints\n",
    "    y_riemann = discontinuous_function(x_riemann[:-1])\n",
    "    \n",
    "    axes[0, 0].plot(x, y, 'b-', linewidth=2, label='Function f(x)')\n",
    "    axes[0, 0].bar(x_riemann[:-1], y_riemann, width=dx, alpha=0.3, \n",
    "                  color='blue', label='Riemann rectangles')\n",
    "    axes[0, 0].set_xlabel('x')\n",
    "    axes[0, 0].set_ylabel('f(x)')\n",
    "    axes[0, 0].set_title('Riemann Integration\\n(Vertical Rectangles)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    riemann_sum = np.sum(y_riemann * dx)\n",
    "    print(f\"   Riemann sum approximation: {riemann_sum:.4f}\")\n",
    "    \n",
    "    # Lebesgue approach: horizontal rectangles\n",
    "    # Group points by function value\n",
    "    y_levels = np.linspace(0, 1, 10)\n",
    "    level_measures = []\n",
    "    \n",
    "    for i in range(len(y_levels) - 1):\n",
    "        y_low, y_high = y_levels[i], y_levels[i + 1]\n",
    "        # Find measure of set where y_low ‚â§ f(x) < y_high\n",
    "        mask = (y >= y_low) & (y < y_high)\n",
    "        measure = np.sum(mask) / len(x)  # Approximate measure\n",
    "        level_measures.append(measure)\n",
    "    \n",
    "    # Plot Lebesgue perspective\n",
    "    cumulative_measure = np.cumsum([0] + level_measures)\n",
    "    \n",
    "    for i, (y_low, y_high) in enumerate(zip(y_levels[:-1], y_levels[1:])):\n",
    "        width = level_measures[i]\n",
    "        if width > 0:\n",
    "            axes[0, 1].barh(y_low, width, height=y_high - y_low, \n",
    "                          alpha=0.6, color=plt.cm.viridis(i/len(y_levels)))\n",
    "    \n",
    "    axes[0, 1].set_ylabel('Function Value')\n",
    "    axes[0, 1].set_xlabel('Measure of Level Set')\n",
    "    axes[0, 1].set_title('Lebesgue Integration\\n(Horizontal Rectangles)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    lebesgue_sum = np.sum([y_levels[i] * level_measures[i] for i in range(len(level_measures))])\n",
    "    print(f\"   Lebesgue sum approximation: {lebesgue_sum:.4f}\")\n",
    "    \n",
    "    # 2. Probability as a measure\n",
    "    print(\"\\n2. Probability as a Special Measure\")\n",
    "    print(\"   P(Œ©) = 1, P(‚àÖ) = 0, countably additive\")\n",
    "    \n",
    "    # Demonstrate probability measure properties\n",
    "    # Simple example: coin flips\n",
    "    outcomes = ['HH', 'HT', 'TH', 'TT']\n",
    "    prob_fair = [0.25, 0.25, 0.25, 0.25]\n",
    "    prob_biased = [0.49, 0.21, 0.21, 0.09]  # P(H) = 0.7\n",
    "    \n",
    "    x_pos = np.arange(len(outcomes))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0, 2].bar(x_pos - width/2, prob_fair, width, \n",
    "                          label='Fair coin', alpha=0.8, color='skyblue')\n",
    "    bars2 = axes[0, 2].bar(x_pos + width/2, prob_biased, width,\n",
    "                          label='Biased coin', alpha=0.8, color='orange')\n",
    "    \n",
    "    axes[0, 2].set_xlabel('Outcome')\n",
    "    axes[0, 2].set_ylabel('Probability')\n",
    "    axes[0, 2].set_title('Probability Measures on Sample Space')\n",
    "    axes[0, 2].set_xticks(x_pos)\n",
    "    axes[0, 2].set_xticklabels(outcomes)\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Verify measure properties\n",
    "    print(f\"   Fair coin: P(Œ©) = {sum(prob_fair)} (should be 1)\")\n",
    "    print(f\"   Biased coin: P(Œ©) = {sum(prob_biased)} (should be 1)\")\n",
    "    \n",
    "    # Event examples\n",
    "    event_at_least_one_H = ['HH', 'HT', 'TH']\n",
    "    p_fair_event = sum(prob_fair[i] for i, outcome in enumerate(outcomes) if outcome in event_at_least_one_H)\n",
    "    p_biased_event = sum(prob_biased[i] for i, outcome in enumerate(outcomes) if outcome in event_at_least_one_H)\n",
    "    \n",
    "    print(f\"   P(at least one H | fair) = {p_fair_event}\")\n",
    "    print(f\"   P(at least one H | biased) = {p_biased_event}\")\n",
    "    \n",
    "    # 3. Random variables as measurable functions\n",
    "    print(\"\\n3. Random Variables: Measurable Functions\")\n",
    "    print(\"   X: (Œ©, ‚Ñ±) ‚Üí (‚Ñù, B(‚Ñù))\")\n",
    "    \n",
    "    # Example: sum of two dice\n",
    "    # Sample space: all pairs (i,j) where i,j ‚àà {1,2,3,4,5,6}\n",
    "    dice_outcomes = [(i, j) for i in range(1, 7) for j in range(1, 7)]\n",
    "    dice_sums = [i + j for i, j in dice_outcomes]\n",
    "    \n",
    "    # Count frequencies\n",
    "    sum_counts = np.bincount(dice_sums)[2:]  # Start from sum=2\n",
    "    sum_values = np.arange(2, 13)\n",
    "    sum_probs = sum_counts / 36\n",
    "    \n",
    "    # Plot probability mass function\n",
    "    bars = axes[1, 0].bar(sum_values, sum_probs, alpha=0.7, color='green')\n",
    "    axes[1, 0].set_xlabel('Sum of Dice (X)')\n",
    "    axes[1, 0].set_ylabel('P(X = x)')\n",
    "    axes[1, 0].set_title('Random Variable: Sum of Two Dice')\n",
    "    axes[1, 0].set_xticks(sum_values)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add probability labels\n",
    "    for bar, prob in zip(bars, sum_probs):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                       f'{prob:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    print(f\"   Sample space size: {len(dice_outcomes)}\")\n",
    "    print(f\"   Range of X: {set(dice_sums)}\")\n",
    "    print(f\"   P(X = 7) = {sum_probs[5]:.3f} (most likely)\")\n",
    "    print(f\"   E[X] = {np.sum(sum_values * sum_probs):.2f}\")\n",
    "    \n",
    "    # 4. Convergence theorems\n",
    "    print(\"\\n4. Convergence Theorems: When Limits Commute with Integrals\")\n",
    "    print(\"   Monotone Convergence, Dominated Convergence, Fatou's Lemma\")\n",
    "    \n",
    "    # Demonstrate Monotone Convergence Theorem\n",
    "    x_conv = np.linspace(0, 2, 1000)\n",
    "    \n",
    "    # Sequence of functions f_n(x) = x^n * 1_{[0,1]}(x)\n",
    "    n_values = [1, 2, 5, 10, 20]\n",
    "    colors_conv = plt.cm.viridis(np.linspace(0, 1, len(n_values)))\n",
    "    \n",
    "    for i, n in enumerate(n_values):\n",
    "        f_n = np.where(x_conv <= 1, x_conv**n, 0)\n",
    "        axes[1, 1].plot(x_conv, f_n, color=colors_conv[i], linewidth=2, \n",
    "                       label=f'$f_{{{n}}}(x) = x^{{{n}}} \\cdot 1_{{[0,1]}}$')\n",
    "    \n",
    "    # Limit function\n",
    "    f_limit = np.where(x_conv < 1, 0, np.where(x_conv == 1, 1, 0))\n",
    "    axes[1, 1].plot(x_conv, f_limit, 'r--', linewidth=3, label='Limit function')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('x')\n",
    "    axes[1, 1].set_ylabel('f_n(x)')\n",
    "    axes[1, 1].set_title('Monotone Convergence Example')\n",
    "    axes[1, 1].legend(fontsize=8)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_ylim(-0.1, 1.1)\n",
    "    \n",
    "    # Calculate integrals\n",
    "    integrals = [1/(n+1) for n in n_values]  # ‚à´‚ÇÄ¬π x^n dx = 1/(n+1)\n",
    "    limit_integral = 0  # ‚à´ limit function = 0\n",
    "    \n",
    "    print(f\"   Integrals of f_n: {[f'{val:.4f}' for val in integrals]}\")\n",
    "    print(f\"   Limit of integrals: {limit_integral} (‚à´ limit function)\")\n",
    "    print(f\"   MCT: lim ‚à´f_n = ‚à´(lim f_n) when f_n ‚Üë\")\n",
    "    \n",
    "    # 5. Radon-Nikodym theorem\n",
    "    print(\"\\n5. Radon-Nikodym Theorem: Existence of Density Functions\")\n",
    "    print(\"   When does dŒΩ = f dŒº exist?\")\n",
    "    \n",
    "    # Example: relationship between different probability measures\n",
    "    x_rn = np.linspace(-3, 3, 1000)\n",
    "    \n",
    "    # Reference measure: standard normal\n",
    "    mu_density = norm.pdf(x_rn, 0, 1)\n",
    "    \n",
    "    # Absolutely continuous measure: shifted normal\n",
    "    nu_density = norm.pdf(x_rn, 1, 1)\n",
    "    \n",
    "    # Radon-Nikodym derivative\n",
    "    rn_derivative = nu_density / mu_density\n",
    "    \n",
    "    axes[1, 2].plot(x_rn, mu_density, 'b-', linewidth=2, label='Œº: N(0,1)')\n",
    "    axes[1, 2].plot(x_rn, nu_density, 'r-', linewidth=2, label='ŒΩ: N(1,1)')\n",
    "    axes[1, 2].plot(x_rn, rn_derivative, 'g--', linewidth=2, label='dŒΩ/dŒº')\n",
    "    axes[1, 2].set_xlabel('x')\n",
    "    axes[1, 2].set_ylabel('Density')\n",
    "    axes[1, 2].set_title('Radon-Nikodym Derivative')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Verify the relationship\n",
    "    # ‚à´ g dŒΩ = ‚à´ g (dŒΩ/dŒº) dŒº for any measurable g\n",
    "    test_function = x_rn**2  # g(x) = x¬≤\n",
    "    \n",
    "    # Numerical integration\n",
    "    dx = x_rn[1] - x_rn[0]\n",
    "    integral_nu = np.sum(test_function * nu_density * dx)\n",
    "    integral_mu_weighted = np.sum(test_function * rn_derivative * mu_density * dx)\n",
    "    \n",
    "    print(f\"   ‚à´ x¬≤ dŒΩ = {integral_nu:.4f}\")\n",
    "    print(f\"   ‚à´ x¬≤ (dŒΩ/dŒº) dŒº = {integral_mu_weighted:.4f}\")\n",
    "    print(f\"   Difference: {abs(integral_nu - integral_mu_weighted):.6f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ Key Measure Theory Concepts:\")\n",
    "    print(\"‚Ä¢ œÉ-algebras ensure mathematical consistency\")\n",
    "    print(\"‚Ä¢ Lebesgue integration generalizes Riemann integration\")\n",
    "    print(\"‚Ä¢ Probability is a special case of measure theory\")\n",
    "    print(\"‚Ä¢ Random variables are measurable functions\")\n",
    "    print(\"‚Ä¢ Convergence theorems enable limit-integral interchange\")\n",
    "    print(\"‚Ä¢ Radon-Nikodym theorem formalizes density functions\")\n",
    "\n",
    "demonstrate_measure_theory_foundations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a132c",
   "metadata": {},
   "source": [
    "## 2. Applications in Machine Learning Theory\n",
    "\n",
    "### PAC Learning and Uniform Convergence\n",
    "**Probably Approximately Correct (PAC)** learning uses measure theory to formalize when learning algorithms will succeed.\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Empirical Risk Minimization (ERM)**: Choose hypothesis minimizing training error\n",
    "- **Uniform Convergence**: Training error converges to true error uniformly over hypothesis class\n",
    "- **VC Dimension**: Measure of hypothesis class complexity\n",
    "\n",
    "### Rademacher Complexity\n",
    "Measures how well a function class can fit random noise:\n",
    "```\n",
    "R_m(‚Ñ±) = E[sup_{f‚àà‚Ñ±} (1/m) Œ£·µ¢ œÉ·µ¢ f(x·µ¢)]\n",
    "```\n",
    "where œÉ·µ¢ are Rademacher random variables (¬±1 with equal probability).\n",
    "\n",
    "### Concentration Inequalities\n",
    "Control how random variables deviate from their expectations:\n",
    "- **Hoeffding's inequality**: For bounded random variables\n",
    "- **McDiarmid's inequality**: For functions with bounded differences\n",
    "- **Azuma's inequality**: For martingales\n",
    "\n",
    "### Reproducing Kernel Hilbert Spaces (RKHS)\n",
    "Functional analysis foundation for kernel methods:\n",
    "- **Hilbert space**: Complete inner product space\n",
    "- **Reproducing property**: ‚ü®k(¬∑,x), f‚ü© = f(x)\n",
    "- **Representer theorem**: Solution lies in span of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_ml_theory_applications():\n",
    "    \"\"\"Explore measure theory applications in ML theory\"\"\"\n",
    "    \n",
    "    print(\"ü§ñ Measure Theory in Machine Learning Theory\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Empirical risk vs true risk\n",
    "    print(\"\\n1. Empirical Risk Minimization\")\n",
    "    print(\"   How training error relates to true error\")\n",
    "    \n",
    "    # Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=2, n_redundant=0,\n",
    "                              n_informative=2, n_clusters_per_class=1, random_state=42)\n",
    "    \n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train models with different complexities\n",
    "    train_sizes = np.logspace(1, np.log10(len(X_train)), 20).astype(int)\n",
    "    \n",
    "    models = {\n",
    "        'Low complexity (RF depth=3)': RandomForestClassifier(max_depth=3, n_estimators=10, random_state=42),\n",
    "        'Medium complexity (RF depth=10)': RandomForestClassifier(max_depth=10, n_estimators=10, random_state=42),\n",
    "        'High complexity (RF depth=None)': RandomForestClassifier(max_depth=None, n_estimators=10, random_state=42)\n",
    "    }\n",
    "    \n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        # Learning curves\n",
    "        train_sizes_actual, train_scores, test_scores = learning_curve(\n",
    "            model, X_train, y_train, train_sizes=train_sizes, cv=3,\n",
    "            scoring='accuracy', random_state=42, n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        train_mean = train_scores.mean(axis=1)\n",
    "        train_std = train_scores.std(axis=1)\n",
    "        test_mean = test_scores.mean(axis=1)\n",
    "        test_std = test_scores.std(axis=1)\n",
    "        \n",
    "        axes[0, 0].plot(train_sizes_actual, train_mean, 'o-', color=colors[i], \n",
    "                       linewidth=2, markersize=4, label=f'{name} (train)')\n",
    "        axes[0, 0].fill_between(train_sizes_actual, train_mean - train_std,\n",
    "                               train_mean + train_std, alpha=0.1, color=colors[i])\n",
    "        \n",
    "        axes[0, 0].plot(train_sizes_actual, test_mean, 's--', color=colors[i],\n",
    "                       linewidth=2, markersize=4, label=f'{name} (test)')\n",
    "        axes[0, 0].fill_between(train_sizes_actual, test_mean - test_std,\n",
    "                               test_mean + test_std, alpha=0.1, color=colors[i])\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Training Set Size')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].set_title('Learning Curves: Empirical vs True Risk')\n",
    "    axes[0, 0].set_xscale('log')\n",
    "    axes[0, 0].legend(fontsize=8)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"   Training samples: {len(X_train)}\")\n",
    "    print(f\"   Test samples: {len(X_test)}\")\n",
    "    print(f\"   Observe: High complexity models overfit with small data\")\n",
    "    \n",
    "    # 2. Concentration inequalities\n",
    "    print(\"\\n2. Concentration Inequalities\")\n",
    "    print(\"   How sample means concentrate around true means\")\n",
    "    \n",
    "    # Demonstrate Hoeffding's inequality\n",
    "    # Generate bounded random variables [0, 1]\n",
    "    true_mean = 0.6\n",
    "    sample_sizes = [10, 50, 100, 500, 1000]\n",
    "    n_experiments = 1000\n",
    "    \n",
    "    deviations = []\n",
    "    hoeffding_bounds = []\n",
    "    \n",
    "    for n in sample_sizes:\n",
    "        # Run experiments\n",
    "        sample_means = []\n",
    "        for _ in range(n_experiments):\n",
    "            samples = np.random.uniform(0, 1, n)  # Bounded in [0,1]\n",
    "            samples = (samples < true_mean).astype(float)  # Convert to Bernoulli\n",
    "            sample_means.append(np.mean(samples))\n",
    "        \n",
    "        sample_means = np.array(sample_means)\n",
    "        empirical_deviation = np.mean(np.abs(sample_means - true_mean))\n",
    "        deviations.append(empirical_deviation)\n",
    "        \n",
    "        # Hoeffding bound: P(|XÃÑ - Œº| ‚â• t) ‚â§ 2exp(-2nt¬≤/(b-a)¬≤)\n",
    "        # For t = expected deviation, solve for the bound\n",
    "        epsilon = 0.1  # We want P(|XÃÑ - Œº| ‚â• Œµ)\n",
    "        hoeffding_bound = 2 * np.exp(-2 * n * epsilon**2)  # (b-a)¬≤ = 1\n",
    "        hoeffding_bounds.append(hoeffding_bound)\n",
    "    \n",
    "    axes[0, 1].loglog(sample_sizes, deviations, 'bo-', linewidth=2, \n",
    "                     markersize=6, label='Empirical deviation')\n",
    "    axes[0, 1].loglog(sample_sizes, hoeffding_bounds, 'r--', linewidth=2,\n",
    "                     label=f'Hoeffding bound (Œµ={epsilon})')\n",
    "    axes[0, 1].loglog(sample_sizes, 1/np.sqrt(np.array(sample_sizes)), 'g:', \n",
    "                     linewidth=2, label='1/‚àön (CLT rate)')\n",
    "    axes[0, 1].set_xlabel('Sample Size (n)')\n",
    "    axes[0, 1].set_ylabel('Deviation from True Mean')\n",
    "    axes[0, 1].set_title('Concentration of Sample Means')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"   True mean: {true_mean}\")\n",
    "    print(f\"   Empirical deviations: {[f'{d:.4f}' for d in deviations]}\")\n",
    "    print(f\"   Hoeffding bounds: {[f'{b:.4f}' for b in hoeffding_bounds]}\")\n",
    "    \n",
    "    # 3. VC dimension illustration\n",
    "    print(\"\\n3. VC Dimension: Measuring Model Complexity\")\n",
    "    print(\"   How many points can a hypothesis class shatter?\")\n",
    "    \n",
    "    # Demonstrate VC dimension for linear classifiers in 2D\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # VC dimension of linear classifiers in 2D is 3\n",
    "    # Show that we can shatter 3 points but not 4\n",
    "    \n",
    "    # 3 points that can be shattered\n",
    "    points_3 = np.array([[0, 0], [1, 0], [0.5, 1]])\n",
    "    \n",
    "    # All possible labelings of 3 points\n",
    "    all_labelings_3 = []\n",
    "    for i in range(2**3):\n",
    "        labeling = [(i >> j) & 1 for j in range(3)]\n",
    "        all_labelings_3.append(labeling)\n",
    "    \n",
    "    # Plot the 3 points\n",
    "    axes[0, 2].scatter(points_3[:, 0], points_3[:, 1], s=100, c='red', \n",
    "                      edgecolors='black', linewidth=2, zorder=5)\n",
    "    \n",
    "    # Annotate points\n",
    "    for i, (x, y) in enumerate(points_3):\n",
    "        axes[0, 2].annotate(f'P{i+1}', (x, y), xytext=(5, 5), \n",
    "                          textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    axes[0, 2].set_xlim(-0.5, 1.5)\n",
    "    axes[0, 2].set_ylim(-0.5, 1.5)\n",
    "    axes[0, 2].set_xlabel('x‚ÇÅ')\n",
    "    axes[0, 2].set_ylabel('x‚ÇÇ')\n",
    "    axes[0, 2].set_title('VC Dimension Example: 3 Points\\n(Can be shattered by linear classifier)')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].set_aspect('equal')\n",
    "    \n",
    "    print(f\"   Linear classifiers in 2D: VC dimension = 3\")\n",
    "    print(f\"   Can represent all {len(all_labelings_3)} labelings of 3 points\")\n",
    "    print(f\"   Cannot shatter any set of 4 points in general position\")\n",
    "    \n",
    "    # 4. Rademacher complexity\n",
    "    print(\"\\n4. Rademacher Complexity\")\n",
    "    print(\"   Measuring how well functions fit random noise\")\n",
    "    \n",
    "    # Empirical Rademacher complexity for linear functions\n",
    "    n_samples_rad = 100\n",
    "    n_trials = 500\n",
    "    dimensions = [1, 2, 5, 10, 20, 50]\n",
    "    \n",
    "    rademacher_complexities = []\n",
    "    theoretical_bounds = []\n",
    "    \n",
    "    for d in dimensions:\n",
    "        rad_values = []\n",
    "        \n",
    "        for trial in range(n_trials):\n",
    "            # Generate random data\n",
    "            X_rad = np.random.randn(n_samples_rad, d)\n",
    "            X_rad = X_rad / np.linalg.norm(X_rad, axis=1, keepdims=True)  # Normalize\n",
    "            \n",
    "            # Generate Rademacher variables\n",
    "            sigma = np.random.choice([-1, 1], size=n_samples_rad)\n",
    "            \n",
    "            # Find linear function that best fits the noise\n",
    "            # This is equivalent to ||X^T œÉ||‚ÇÇ / n\n",
    "            rademacher_value = np.linalg.norm(X_rad.T @ sigma) / n_samples_rad\n",
    "            rad_values.append(rademacher_value)\n",
    "        \n",
    "        empirical_rad = np.mean(rad_values)\n",
    "        rademacher_complexities.append(empirical_rad)\n",
    "        \n",
    "        # Theoretical bound: R_m(linear functions) ‚â§ ‚àö(d/m)\n",
    "        theoretical_bound = np.sqrt(d / n_samples_rad)\n",
    "        theoretical_bounds.append(theoretical_bound)\n",
    "    \n",
    "    axes[1, 0].loglog(dimensions, rademacher_complexities, 'bo-', linewidth=2,\n",
    "                     markersize=6, label='Empirical Rademacher complexity')\n",
    "    axes[1, 0].loglog(dimensions, theoretical_bounds, 'r--', linewidth=2,\n",
    "                     label='Theoretical bound ‚àö(d/m)')\n",
    "    axes[1, 0].set_xlabel('Dimension (d)')\n",
    "    axes[1, 0].set_ylabel('Rademacher Complexity')\n",
    "    axes[1, 0].set_title(f'Rademacher Complexity vs Dimension\\n(n={n_samples_rad} samples)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"   Sample size: {n_samples_rad}\")\n",
    "    print(f\"   Empirical complexities: {[f'{r:.4f}' for r in rademacher_complexities[:4]]}...\")\n",
    "    print(f\"   Theoretical bounds: {[f'{t:.4f}' for t in theoretical_bounds[:4]]}...\")\n",
    "    \n",
    "    # 5. Kernel methods and RKHS\n",
    "    print(\"\\n5. Reproducing Kernel Hilbert Spaces\")\n",
    "    print(\"   Functional analysis foundation of kernel methods\")\n",
    "    \n",
    "    # Demonstrate different kernels\n",
    "    from sklearn.metrics.pairwise import rbf_kernel, polynomial_kernel\n",
    "    \n",
    "    # Generate 1D data\n",
    "    x_kernel = np.linspace(-2, 2, 100).reshape(-1, 1)\n",
    "    x_train_kernel = np.array([-1.5, -0.5, 0.5, 1.5]).reshape(-1, 1)\n",
    "    y_train_kernel = np.array([1, -1, 1, -1])\n",
    "    \n",
    "    # Different kernels\n",
    "    kernels = {\n",
    "        'RBF (Œ≥=1)': lambda X1, X2: rbf_kernel(X1, X2, gamma=1),\n",
    "        'RBF (Œ≥=5)': lambda X1, X2: rbf_kernel(X1, X2, gamma=5),\n",
    "        'Polynomial (d=2)': lambda X1, X2: polynomial_kernel(X1, X2, degree=2, coef0=1)\n",
    "    }\n",
    "    \n",
    "    colors_kernel = ['blue', 'green', 'red']\n",
    "    \n",
    "    for i, (name, kernel_func) in enumerate(kernels.items()):\n",
    "        # Compute kernel matrix\n",
    "        K_train = kernel_func(x_train_kernel, x_train_kernel)\n",
    "        K_test = kernel_func(x_kernel, x_train_kernel)\n",
    "        \n",
    "        # Solve for dual coefficients (simplified - no regularization)\n",
    "        try:\n",
    "            alpha = np.linalg.solve(K_train, y_train_kernel)\n",
    "            # Prediction: f(x) = Œ£ Œ±·µ¢ k(x, x·µ¢)\n",
    "            y_pred = K_test @ alpha\n",
    "            \n",
    "            axes[1, 1].plot(x_kernel.flatten(), y_pred, color=colors_kernel[i],\n",
    "                           linewidth=2, label=name)\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(f\"   Warning: Singular kernel matrix for {name}\")\n",
    "    \n",
    "    # Plot training data\n",
    "    axes[1, 1].scatter(x_train_kernel.flatten(), y_train_kernel, \n",
    "                      c='black', s=100, edgecolors='white', linewidth=2,\n",
    "                      zorder=5, label='Training data')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('x')\n",
    "    axes[1, 1].set_ylabel('f(x)')\n",
    "    axes[1, 1].set_title('Kernel Functions in RKHS')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"   Training points: {x_train_kernel.flatten()}\")\n",
    "    print(f\"   Training labels: {y_train_kernel}\")\n",
    "    print(f\"   Different kernels create different function spaces\")\n",
    "    \n",
    "    # 6. Generalization bounds\n",
    "    print(\"\\n6. Generalization Bounds\")\n",
    "    print(\"   Connecting empirical risk to true risk\")\n",
    "    \n",
    "    # Demonstrate how bounds depend on sample size and complexity\n",
    "    sample_sizes_gen = np.logspace(1, 4, 50)\n",
    "    vc_dimensions = [5, 10, 20, 50]\n",
    "    confidence = 0.05  # 95% confidence\n",
    "    \n",
    "    for i, vc_dim in enumerate(vc_dimensions):\n",
    "        # VC generalization bound (simplified)\n",
    "        # With probability ‚â• 1-Œ¥: |R(h) - RÃÇ(h)| ‚â§ ‚àö((VC_dim * log(m/VC_dim) + log(1/Œ¥)) / m)\n",
    "        bounds = []\n",
    "        for m in sample_sizes_gen:\n",
    "            if m > vc_dim:  # Valid regime\n",
    "                bound = np.sqrt((vc_dim * np.log(m/vc_dim) + np.log(1/confidence)) / m)\n",
    "                bounds.append(bound)\n",
    "            else:\n",
    "                bounds.append(np.nan)\n",
    "        \n",
    "        axes[1, 2].loglog(sample_sizes_gen, bounds, linewidth=2,\n",
    "                         label=f'VC dim = {vc_dim}')\n",
    "    \n",
    "    axes[1, 2].set_xlabel('Sample Size (m)')\n",
    "    axes[1, 2].set_ylabel('Generalization Bound')\n",
    "    axes[1, 2].set_title('VC Generalization Bounds')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"   Confidence level: {1-confidence:.0%}\")\n",
    "    print(f\"   Bounds decrease as O(‚àö(VC_dim * log(m) / m))\")\n",
    "    print(f\"   Higher complexity ‚Üí looser bounds\")\n",
    "    print(f\"   More data ‚Üí tighter bounds\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ Key ML Theory Concepts:\")\n",
    "    print(\"‚Ä¢ ERM works when empirical risk uniformly converges to true risk\")\n",
    "    print(\"‚Ä¢ Concentration inequalities bound deviations from expectations\")\n",
    "    print(\"‚Ä¢ VC dimension measures the complexity of hypothesis classes\")\n",
    "    print(\"‚Ä¢ Rademacher complexity measures overfitting to random noise\")\n",
    "    print(\"‚Ä¢ RKHS provides the functional analysis foundation for kernels\")\n",
    "    print(\"‚Ä¢ Generalization bounds connect sample complexity to model complexity\")\n",
    "\n",
    "demonstrate_ml_theory_applications()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720e0b27",
   "metadata": {},
   "source": [
    "## Conclusion: The Mathematical Foundation\n",
    "\n",
    "### What We've Accomplished\n",
    "Through this journey into **applied measure theory**, you've built the rigorous mathematical foundation that underlies modern machine learning theory. You now understand:\n",
    "\n",
    "#### Core Measure Theory\n",
    "- **Measure spaces**: The triple (Œ©, ‚Ñ±, Œº) that makes probability rigorous\n",
    "- **Lebesgue integration**: A more general approach than Riemann integration\n",
    "- **Random variables**: Measurable functions from probability spaces to real numbers\n",
    "- **Convergence theorems**: When we can interchange limits and integrals\n",
    "\n",
    "#### Machine Learning Applications\n",
    "- **PAC learning**: Formal framework for learnable problems\n",
    "- **Uniform convergence**: When empirical risk converges to true risk\n",
    "- **VC dimension**: Measuring the complexity of hypothesis classes\n",
    "- **Concentration inequalities**: Controlling deviations from expectations\n",
    "- **RKHS theory**: The functional analysis behind kernel methods\n",
    "\n",
    "### Why This Matters\n",
    "Measure theory isn't just abstract mathematics‚Äîit's the **invisible foundation** that makes machine learning theoretically sound:\n",
    "\n",
    "1. **Rigorous probability**: Avoids paradoxes and logical inconsistencies\n",
    "2. **Generalization theory**: Explains why ML algorithms work\n",
    "3. **Kernel methods**: Provides the mathematical framework for SVMs and Gaussian processes\n",
    "4. **Deep learning theory**: Enables analysis of neural network expressivity and generalization\n",
    "\n",
    "### The Broader Picture\n",
    "You've now completed a comprehensive journey through the mathematical foundations of machine learning:\n",
    "\n",
    "- **Linear algebra**: The computational engine\n",
    "- **Calculus**: The optimization toolkit\n",
    "- **Probability**: The uncertainty framework\n",
    "- **Statistics**: The inference methodology\n",
    "- **Optimization**: The learning algorithms\n",
    "- **Information theory**: The complexity measures\n",
    "- **Bayesian methods**: The uncertainty quantification\n",
    "- **Stochastic processes**: The temporal modeling\n",
    "- **Measure theory**: The rigorous foundation\n",
    "\n",
    "### Moving Forward\n",
    "With these mathematical tools, you're equipped to:\n",
    "- Understand cutting-edge ML research papers\n",
    "- Develop new algorithms with theoretical guarantees\n",
    "- Debug and improve existing methods\n",
    "- Bridge the gap between theory and practice\n",
    "\n",
    "Remember: **mathematics is not the enemy of intuition‚Äîit's the language that makes intuition precise and reliable.**\n",
    "\n",
    "üéì **Congratulations on mastering the mathematical foundations of machine learning!** üéì"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
