{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Calculus Foundations for Machine Learning\n",
    "\n",
    "This notebook covers essential calculus concepts for optimization in machine learning:\n",
    "- Partial derivatives and gradients\n",
    "- Optimization fundamentals\n",
    "- Gradient descent algorithms\n",
    "- Chain rule and backpropagation\n",
    "- Hessian matrices and second-order optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.optimize import minimize\n",
    "import sympy as sp\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Gradients\n",
    "\n",
    "The gradient points in the direction of steepest ascent and is fundamental to optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 2D function and its gradient\n",
    "def f(x, y):\n",
    "    \"\"\"Example function: f(x,y) = x^2 + 2y^2 - 2xy + 2x\"\"\"\n",
    "    return x**2 + 2*y**2 - 2*x*y + 2*x\n",
    "\n",
    "def gradient_f(x, y):\n",
    "    \"\"\"Gradient of f: [∂f/∂x, ∂f/∂y]\"\"\"\n",
    "    df_dx = 2*x - 2*y + 2\n",
    "    df_dy = 4*y - 2*x\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# Create a grid for visualization\n",
    "x = np.linspace(-3, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# Create gradient field\n",
    "x_grad = np.linspace(-3, 2, 20)\n",
    "y_grad = np.linspace(-2, 2, 20)\n",
    "X_grad, Y_grad = np.meshgrid(x_grad, y_grad)\n",
    "U, V = gradient_f(X_grad, Y_grad)\n",
    "\n",
    "# Plot function and gradient field\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Contour plot with gradient field\n",
    "contour = ax1.contour(X, Y, Z, levels=20, alpha=0.7)\n",
    "ax1.clabel(contour, inline=True, fontsize=8)\n",
    "ax1.quiver(X_grad, Y_grad, U, V, angles='xy', scale_units='xy', scale=1, alpha=0.7, color='red')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Function Contours with Gradient Field', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 3D surface plot\n",
    "ax2 = plt.axes(projection='3d')\n",
    "surf = ax2.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_zlabel('f(x,y)')\n",
    "ax2.set_title('3D Surface Plot', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find critical points\n",
    "print(\"Finding critical points where gradient = 0:\")\n",
    "print(\"∂f/∂x = 2x - 2y + 2 = 0\")\n",
    "print(\"∂f/∂y = 4y - 2x = 0\")\n",
    "print(\"\\nSolving: x = 2y and x - y = -1\")\n",
    "print(\"Critical point: x = -2, y = -1\")\n",
    "critical_point = np.array([-2, -1])\n",
    "print(f\"Function value at critical point: f(-2, -1) = {f(critical_point[0], critical_point[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent Algorithm\n",
    "\n",
    "The foundation of training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(start_point, learning_rate=0.1, max_iterations=100, tolerance=1e-6):\n",
    "    \"\"\"Gradient descent optimization\"\"\"\n",
    "    path = [start_point.copy()]\n",
    "    point = start_point.copy()\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        grad = gradient_f(point[0], point[1])\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(grad) < tolerance:\n",
    "            print(f\"Converged after {i} iterations\")\n",
    "            break\n",
    "            \n",
    "        # Update point\n",
    "        point = point - learning_rate * grad\n",
    "        path.append(point.copy())\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# Test different starting points and learning rates\n",
    "start_points = [np.array([2, 1]), np.array([-1, 2]), np.array([0, 0])]\n",
    "learning_rates = [0.05, 0.1, 0.2]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, lr in enumerate(learning_rates):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot contours\n",
    "    contour = ax.contour(X, Y, Z, levels=20, alpha=0.7)\n",
    "    \n",
    "    colors = ['red', 'blue', 'green']\n",
    "    \n",
    "    for i, start in enumerate(start_points):\n",
    "        path = gradient_descent(start, learning_rate=lr, max_iterations=50)\n",
    "        \n",
    "        # Plot optimization path\n",
    "        ax.plot(path[:, 0], path[:, 1], 'o-', color=colors[i], \n",
    "               linewidth=2, markersize=4, alpha=0.8,\n",
    "               label=f'Start: ({start[0]}, {start[1]})')\n",
    "        \n",
    "        # Mark start and end points\n",
    "        ax.plot(start[0], start[1], 's', color=colors[i], markersize=8)\n",
    "        ax.plot(path[-1, 0], path[-1, 1], '*', color=colors[i], markersize=12)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'Learning Rate: {lr}', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Gradient Descent with Different Learning Rates', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chain Rule and Backpropagation\n",
    "\n",
    "Understanding how gradients flow through neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a simple neural network with chain rule\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(2, 3) * 0.1  # Input to hidden\n",
    "        self.b1 = np.zeros((1, 3))\n",
    "        self.W2 = np.random.randn(3, 1) * 0.1  # Hidden to output\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "        \n",
    "        # Store intermediate values for backprop\n",
    "        self.cache = {}\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward pass with caching for backprop\n",
    "        self.cache['X'] = X\n",
    "        \n",
    "        # Layer 1\n",
    "        self.cache['Z1'] = X @ self.W1 + self.b1\n",
    "        self.cache['A1'] = self.sigmoid(self.cache['Z1'])\n",
    "        \n",
    "        # Layer 2 (output)\n",
    "        self.cache['Z2'] = self.cache['A1'] @ self.W2 + self.b2\n",
    "        self.cache['A2'] = self.sigmoid(self.cache['Z2'])\n",
    "        \n",
    "        return self.cache['A2']\n",
    "    \n",
    "    def backward(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ2 = y_pred - y_true\n",
    "        dW2 = (1/m) * self.cache['A1'].T @ dZ2\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradients (chain rule)\n",
    "        dA1 = dZ2 @ self.W2.T\n",
    "        dZ1 = dA1 * self.sigmoid_derivative(self.cache['Z1'])\n",
    "        dW1 = (1/m) * self.cache['X'].T @ dZ1\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "        \n",
    "        return {\n",
    "            'dW1': dW1, 'db1': db1,\n",
    "            'dW2': dW2, 'db2': db2\n",
    "        }\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(float).reshape(-1, 1)\n",
    "\n",
    "# Train the network\n",
    "nn = SimpleNeuralNetwork()\n",
    "learning_rate = 0.5\n",
    "epochs = 1000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    y_pred = nn.forward(X)\n",
    "    \n",
    "    # Compute loss (binary cross-entropy)\n",
    "    loss = -np.mean(y * np.log(y_pred + 1e-8) + (1 - y) * np.log(1 - y_pred + 1e-8))\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    gradients = nn.backward(y, y_pred)\n",
    "    \n",
    "    # Update parameters\n",
    "    nn.W1 -= learning_rate * gradients['dW1']\n",
    "    nn.b1 -= learning_rate * gradients['db1']\n",
    "    nn.W2 -= learning_rate * gradients['dW2']\n",
    "    nn.b2 -= learning_rate * gradients['db2']\n",
    "\n",
    "# Visualize training progress and decision boundary\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot training loss\n",
    "ax1.plot(losses, linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss (Backpropagation)', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z_pred = nn.forward(grid_points).reshape(xx.shape)\n",
    "\n",
    "ax2.contourf(xx, yy, Z_pred, levels=50, alpha=0.8, cmap='RdBu')\n",
    "scatter = ax2.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap='RdBu', edgecolors='black')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "ax2.set_title('Learned Decision Boundary', fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Accuracy: {np.mean((y_pred > 0.5) == y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization Landscapes\n",
    "\n",
    "Understanding different types of optimization challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different optimization landscapes\n",
    "def convex_function(x, y):\n",
    "    \"\"\"Simple convex function\"\"\"\n",
    "    return x**2 + y**2\n",
    "\n",
    "def non_convex_function(x, y):\n",
    "    \"\"\"Non-convex function with multiple local minima\"\"\"\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2  # Himmelblau's function\n",
    "\n",
    "def saddle_function(x, y):\n",
    "    \"\"\"Function with saddle point\"\"\"\n",
    "    return x**2 - y**2\n",
    "\n",
    "# Create grids\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "functions = [\n",
    "    (convex_function, \"Convex Function (Global Minimum)\"),\n",
    "    (non_convex_function, \"Non-convex Function (Multiple Local Minima)\"),\n",
    "    (saddle_function, \"Saddle Point Function\")\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "for idx, (func, title) in enumerate(functions):\n",
    "    Z = func(X, Y)\n",
    "    \n",
    "    # 2D contour plot\n",
    "    ax_2d = axes[0, idx]\n",
    "    contour = ax_2d.contour(X, Y, Z, levels=20)\n",
    "    ax_2d.clabel(contour, inline=True, fontsize=8)\n",
    "    ax_2d.set_title(f'{title} (Contour)', fontweight='bold')\n",
    "    ax_2d.set_xlabel('x')\n",
    "    ax_2d.set_ylabel('y')\n",
    "    ax_2d.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3D surface plot\n",
    "    ax_3d = plt.subplot(2, 3, idx + 4, projection='3d')\n",
    "    if idx == 1:  # Non-convex function - use log scale for better visualization\n",
    "        Z_plot = np.log(Z + 1)\n",
    "    else:\n",
    "        Z_plot = Z\n",
    "    \n",
    "    surf = ax_3d.plot_surface(X, Y, Z_plot, cmap='viridis', alpha=0.8)\n",
    "    ax_3d.set_title(f'{title} (3D)', fontweight='bold')\n",
    "    ax_3d.set_xlabel('x')\n",
    "    ax_3d.set_ylabel('y')\n",
    "    ax_3d.set_zlabel('f(x,y)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Optimization: Momentum and Adam\n",
    "\n",
    "Modern optimization algorithms that improve upon basic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationAlgorithms:\n",
    "    @staticmethod\n",
    "    def gradient_descent(start_point, gradient_func, learning_rate=0.01, iterations=100):\n",
    "        path = [start_point.copy()]\n",
    "        point = start_point.copy()\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            grad = gradient_func(point[0], point[1])\n",
    "            point = point - learning_rate * grad\n",
    "            path.append(point.copy())\n",
    "        \n",
    "        return np.array(path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def momentum(start_point, gradient_func, learning_rate=0.01, momentum=0.9, iterations=100):\n",
    "        path = [start_point.copy()]\n",
    "        point = start_point.copy()\n",
    "        velocity = np.zeros_like(point)\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            grad = gradient_func(point[0], point[1])\n",
    "            velocity = momentum * velocity + learning_rate * grad\n",
    "            point = point - velocity\n",
    "            path.append(point.copy())\n",
    "        \n",
    "        return np.array(path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def adam(start_point, gradient_func, learning_rate=0.01, beta1=0.9, beta2=0.999, \n",
    "             epsilon=1e-8, iterations=100):\n",
    "        path = [start_point.copy()]\n",
    "        point = start_point.copy()\n",
    "        m = np.zeros_like(point)  # First moment\n",
    "        v = np.zeros_like(point)  # Second moment\n",
    "        \n",
    "        for t in range(1, iterations + 1):\n",
    "            grad = gradient_func(point[0], point[1])\n",
    "            \n",
    "            # Update moments\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            v = beta2 * v + (1 - beta2) * grad**2\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = m / (1 - beta1**t)\n",
    "            v_hat = v / (1 - beta2**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            point = point - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "            path.append(point.copy())\n",
    "        \n",
    "        return np.array(path)\n",
    "\n",
    "# Test on a challenging function (elongated valley)\n",
    "def challenging_function(x, y):\n",
    "    return 100 * (y - x**2)**2 + (1 - x)**2  # Rosenbrock function\n",
    "\n",
    "def gradient_challenging(x, y):\n",
    "    df_dx = -400 * x * (y - x**2) - 2 * (1 - x)\n",
    "    df_dy = 200 * (y - x**2)\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# Starting point\n",
    "start = np.array([-1.0, 1.0])\n",
    "iterations = 200\n",
    "\n",
    "# Run different optimizers\n",
    "optimizers = {\n",
    "    'Gradient Descent': OptimizationAlgorithms.gradient_descent(\n",
    "        start, gradient_challenging, learning_rate=0.001, iterations=iterations),\n",
    "    'Momentum': OptimizationAlgorithms.momentum(\n",
    "        start, gradient_challenging, learning_rate=0.001, momentum=0.9, iterations=iterations),\n",
    "    'Adam': OptimizationAlgorithms.adam(\n",
    "        start, gradient_challenging, learning_rate=0.01, iterations=iterations)\n",
    "}\n",
    "\n",
    "# Create visualization\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "y_range = np.linspace(-1, 3, 100)\n",
    "X_viz, Y_viz = np.meshgrid(x_range, y_range)\n",
    "Z_viz = challenging_function(X_viz, Y_viz)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot optimization paths\n",
    "colors = ['red', 'blue', 'green']\n",
    "contour = ax1.contour(X_viz, Y_viz, np.log(Z_viz + 1), levels=20, alpha=0.7)\n",
    "\n",
    "for (name, path), color in zip(optimizers.items(), colors):\n",
    "    ax1.plot(path[:, 0], path[:, 1], color=color, linewidth=2, alpha=0.8, label=name)\n",
    "    ax1.plot(path[0, 0], path[0, 1], 'o', color=color, markersize=8)  # Start\n",
    "    ax1.plot(path[-1, 0], path[-1, 1], '*', color=color, markersize=12)  # End\n",
    "\n",
    "ax1.plot(1, 1, 'k*', markersize=15, label='Global Minimum')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Optimization Paths (Rosenbrock Function)', fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot convergence curves\n",
    "for (name, path), color in zip(optimizers.items(), colors):\n",
    "    function_values = [challenging_function(p[0], p[1]) for p in path]\n",
    "    ax2.semilogy(function_values, color=color, linewidth=2, label=name)\n",
    "\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Function Value (log scale)')\n",
    "ax2.set_title('Convergence Comparison', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(\"Final positions and function values:\")\n",
    "for name, path in optimizers.items():\n",
    "    final_point = path[-1]\n",
    "    final_value = challenging_function(final_point[0], final_point[1])\n",
    "    print(f\"{name}: ({final_point[0]:.4f}, {final_point[1]:.4f}), f = {final_value:.6f}\")\n",
    "\n",
    "print(f\"\\nGlobal minimum: (1.0, 1.0), f = 0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hessian Matrix and Second-Order Methods\n",
    "\n",
    "Understanding curvature information for advanced optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Hessian matrix numerically\n",
    "def compute_hessian_numerical(func, point, h=1e-5):\n",
    "    \"\"\"Compute Hessian matrix using finite differences\"\"\"\n",
    "    x, y = point\n",
    "    \n",
    "    # Second partial derivatives\n",
    "    fxx = (func(x + h, y) - 2*func(x, y) + func(x - h, y)) / h**2\n",
    "    fyy = (func(x, y + h) - 2*func(x, y) + func(x, y - h)) / h**2\n",
    "    fxy = (func(x + h, y + h) - func(x + h, y - h) - func(x - h, y + h) + func(x - h, y - h)) / (4 * h**2)\n",
    "    \n",
    "    return np.array([[fxx, fxy], [fxy, fyy]])\n",
    "\n",
    "# Analytical Hessian for our quadratic function\n",
    "def hessian_f(x, y):\n",
    "    \"\"\"Analytical Hessian of f(x,y) = x^2 + 2y^2 - 2xy + 2x\"\"\"\n",
    "    return np.array([[2, -2], [-2, 4]])\n",
    "\n",
    "# Test point\n",
    "test_point = np.array([1.0, 0.5])\n",
    "\n",
    "# Compare numerical and analytical Hessians\n",
    "H_numerical = compute_hessian_numerical(f, test_point)\n",
    "H_analytical = hessian_f(test_point[0], test_point[1])\n",
    "\n",
    "print(\"Hessian Matrix Analysis\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Test point: ({test_point[0]}, {test_point[1]})\")\n",
    "print(f\"\\nNumerical Hessian:\\n{H_numerical}\")\n",
    "print(f\"\\nAnalytical Hessian:\\n{H_analytical}\")\n",
    "print(f\"\\nDifference:\\n{H_numerical - H_analytical}\")\n",
    "\n",
    "# Eigenvalue analysis\n",
    "eigenvals, eigenvecs = np.linalg.eig(H_analytical)\n",
    "print(f\"\\nEigenvalues: {eigenvals}\")\n",
    "print(f\"Eigenvectors:\\n{eigenvecs}\")\n",
    "\n",
    "# Classify critical point\n",
    "if np.all(eigenvals > 0):\n",
    "    classification = \"Local minimum (positive definite)\"\n",
    "elif np.all(eigenvals < 0):\n",
    "    classification = \"Local maximum (negative definite)\"\n",
    "else:\n",
    "    classification = \"Saddle point (indefinite)\"\n",
    "\n",
    "print(f\"\\nPoint classification: {classification}\")\n",
    "\n",
    "# Visualize quadratic approximation\n",
    "def quadratic_approximation(center, grad, hessian, x, y):\n",
    "    \"\"\"Second-order Taylor approximation\"\"\"\n",
    "    dx = x - center[0]\n",
    "    dy = y - center[1]\n",
    "    delta = np.array([dx, dy])\n",
    "    \n",
    "    f_center = f(center[0], center[1])\n",
    "    linear_term = grad[0] * dx + grad[1] * dy\n",
    "    quadratic_term = 0.5 * (delta @ hessian @ delta.T)\n",
    "    \n",
    "    return f_center + linear_term + quadratic_term\n",
    "\n",
    "# Create comparison plot\n",
    "center = np.array([0.0, 0.0])\n",
    "grad_center = gradient_f(center[0], center[1])\n",
    "hess_center = hessian_f(center[0], center[1])\n",
    "\n",
    "x_approx = np.linspace(-1, 1, 50)\n",
    "y_approx = np.linspace(-1, 1, 50)\n",
    "X_approx, Y_approx = np.meshgrid(x_approx, y_approx)\n",
    "\n",
    "Z_original = f(X_approx, Y_approx)\n",
    "Z_approx = quadratic_approximation(center, grad_center, hess_center, X_approx, Y_approx)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Original function\n",
    "contour1 = axes[0].contour(X_approx, Y_approx, Z_original, levels=15)\n",
    "axes[0].clabel(contour1, inline=True, fontsize=8)\n",
    "axes[0].plot(center[0], center[1], 'ro', markersize=10, label='Center')\n",
    "axes[0].set_title('Original Function', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Quadratic approximation\n",
    "contour2 = axes[1].contour(X_approx, Y_approx, Z_approx, levels=15)\n",
    "axes[1].clabel(contour2, inline=True, fontsize=8)\n",
    "axes[1].plot(center[0], center[1], 'ro', markersize=10, label='Center')\n",
    "axes[1].set_title('Quadratic Approximation', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Error\n",
    "Z_error = np.abs(Z_original - Z_approx)\n",
    "contour3 = axes[2].contour(X_approx, Y_approx, Z_error, levels=15)\n",
    "axes[2].clabel(contour3, inline=True, fontsize=8)\n",
    "axes[2].plot(center[0], center[1], 'ro', markersize=10, label='Center')\n",
    "axes[2].set_title('Approximation Error', fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Optimization Visualization\n",
    "\n",
    "Explore how different parameters affect optimization behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive visualization with Plotly\n",
    "def create_optimization_surface():\n",
    "    \"\"\"Create interactive 3D optimization surface\"\"\"\n",
    "    x = np.linspace(-3, 3, 50)\n",
    "    y = np.linspace(-3, 3, 50)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    # Create surface plot\n",
    "    fig = go.Figure(data=[go.Surface(z=Z, x=X, y=Y, \n",
    "                                   colorscale='Viridis',\n",
    "                                   name='Function Surface')])\n",
    "    \n",
    "    # Add optimization path\n",
    "    start_point = np.array([2.0, 1.5])\n",
    "    path = gradient_descent(start_point, learning_rate=0.1, max_iterations=50)\n",
    "    \n",
    "    path_z = [f(p[0], p[1]) for p in path]\n",
    "    \n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=path[:, 0], y=path[:, 1], z=path_z,\n",
    "        mode='markers+lines',\n",
    "        marker=dict(size=5, color='red'),\n",
    "        line=dict(color='red', width=5),\n",
    "        name='Optimization Path'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Interactive Optimization Surface',\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='f(x,y)'\n",
    "        ),\n",
    "        width=800, height=600\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display interactive plot\n",
    "interactive_fig = create_optimization_surface()\n",
    "interactive_fig.show()\n",
    "\n",
    "# Create learning rate comparison\n",
    "def compare_learning_rates():\n",
    "    \"\"\"Compare different learning rates\"\"\"\n",
    "    learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
    "    start_point = np.array([2.0, 1.5])\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    colors = ['blue', 'green', 'red', 'orange']\n",
    "    \n",
    "    for lr, color in zip(learning_rates, colors):\n",
    "        path = gradient_descent(start_point, learning_rate=lr, max_iterations=50)\n",
    "        losses = [f(p[0], p[1]) for p in path]\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(losses))),\n",
    "            y=losses,\n",
    "            mode='lines+markers',\n",
    "            name=f'LR = {lr}',\n",
    "            line=dict(color=color, width=2)\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Learning Rate Comparison',\n",
    "        xaxis_title='Iteration',\n",
    "        yaxis_title='Function Value',\n",
    "        yaxis_type='log',\n",
    "        width=800, height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "lr_comparison = compare_learning_rates()\n",
    "lr_comparison.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Gradients**: Point in direction of steepest ascent, fundamental for optimization\n",
    "2. **Gradient Descent**: Iterative optimization by following negative gradient\n",
    "3. **Learning Rate**: Critical hyperparameter affecting convergence speed and stability\n",
    "4. **Chain Rule**: Enables backpropagation in neural networks\n",
    "5. **Advanced Optimizers**: Momentum and Adam improve upon basic gradient descent\n",
    "6. **Hessian Matrix**: Provides curvature information for second-order methods\n",
    "7. **Optimization Landscapes**: Different functions present unique challenges\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Implement different optimization algorithms\n",
    "- Study convex optimization theory\n",
    "- Explore constrained optimization\n",
    "- Practice with real machine learning loss functions\n",
    "- Learn about automatic differentiation\n",
    "\n",
    "---\n",
    "\n",
    "**Mathematical Foundations Covered:**\n",
    "- Partial derivatives and gradients\n",
    "- Chain rule for composite functions\n",
    "- Taylor series approximations\n",
    "- Eigenvalue decomposition\n",
    "- Optimization theory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}